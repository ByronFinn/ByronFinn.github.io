<!doctype html><html lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Transformer架构深度解析：注意力机制与AI大模型的核心技术 - 每日深度思考 | 技术、经济分析与深度思考</title><meta name=Description content="深入解析Transformer架构核心技术：自注意力机制、多头注意力、位置编码等核心组件。详细阐述Query/Key/Value原理，理解GPT、BERT等大模型的技术基础，掌握现代AI的核心架构。"><meta property="og:url" content="https://byronfinn.github.io/ai%E6%95%99%E7%A8%8B2/"><meta property="og:site_name" content="每日深度思考 | 技术、经济分析与深度思考"><meta property="og:title" content="Transformer架构深度解析：注意力机制与AI大模型的核心技术"><meta property="og:description" content="深入解析Transformer架构核心技术：自注意力机制、多头注意力、位置编码等核心组件。详细阐述Query/Key/Value原理，理解GPT、BERT等大模型的技术基础，掌握现代AI的核心架构。"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-11-05T09:58:24+08:00"><meta property="article:modified_time" content="2025-11-06T13:19:28+08:00"><meta property="article:tag" content="Transformer架构"><meta property="article:tag" content="注意力机制"><meta property="article:tag" content="自注意力"><meta property="article:tag" content="多头注意力"><meta property="article:tag" content="Query Key Value"><meta property="article:tag" content="位置编码"><meta property="og:image" content="https://byronfinn.github.io/static/avatar/angryCat.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://byronfinn.github.io/static/avatar/angryCat.png"><meta name=twitter:title content="Transformer架构深度解析：注意力机制与AI大模型的核心技术"><meta name=twitter:description content="深入解析Transformer架构核心技术：自注意力机制、多头注意力、位置编码等核心组件。详细阐述Query/Key/Value原理，理解GPT、BERT等大模型的技术基础，掌握现代AI的核心架构。"><meta name=application-name content="Daily Deep Think"><meta name=apple-mobile-web-app-title content="Daily Deep Think"><meta name=theme-color content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel=icon href=/static/icos/favicon.svg><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://byronfinn.github.io/ai%E6%95%99%E7%A8%8B2/><link rel=prev href=https://byronfinn.github.io/ai%E6%95%99%E7%A8%8B3/><link rel=next href=https://byronfinn.github.io/ai%E6%95%99%E7%A8%8B1/><link rel=stylesheet href=/css/main.min.css><link rel=stylesheet href=/css/style.min.css><meta name=google-site-verification content="google8f3e688b6959e353"><meta name=msvalidate.01 content="请在此添加你的Bing验证码"><meta name=yandex-verification content="请在此添加你的Yandex验证码"><meta name=p:domain_verify content="请在此添加你的Pinterest验证码"><meta name=baidu-site-verification content="请在此添加你的百度验证码"><meta name=sogou_site_verification content="请在此添加你的搜狗验证码"><meta name=360-site-verification content="请在此添加你的360搜索验证码"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Transformer架构深度解析：注意力机制与AI大模型的核心技术","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https://byronfinn.github.io/ai%E6%95%99%E7%A8%8B2/"},"image":["https://byronfinn.github.io/static/avatar/angryCat.png"],"genre":"posts","keywords":["Transformer","注意力机制","自注意力","多头注意力","Query Key Value","位置编码","AI架构","深度学习","神经网络","GPT","BERT"],"wordcount":3748,"url":"https://byronfinn.github.io/ai%E6%95%99%E7%A8%8B2/","datePublished":"2025-11-05T09:58:24+08:00","dateModified":"2025-11-06T13:19:28+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"Finn","url":"https://blog.baifan.site"},"description":"深入解析Transformer架构核心技术：自注意力机制、多头注意力、位置编码等核心组件。详细阐述Query/Key/Value原理，理解GPT、BERT等大模型的技术基础，掌握现代AI的核心架构。"}</script></head><body data-instant-intensity=viewport class="tw-flex tw-min-h-screen tw-flex-col"><script>function setTheme(e){document.body.setAttribute("theme",e),document.documentElement.className=e,document.documentElement.style.setProperty("color-scheme",e==="light"?"light":"dark"),e==="light"?document.documentElement.classList.remove("tw-dark"):document.documentElement.classList.add("tw-dark"),window.theme=e,window.isDark=window.theme!=="light"}function saveTheme(e){window.localStorage&&localStorage.setItem("theme",e)}function getMeta(e){const t=document.getElementsByTagName("meta");for(let n=0;n<t.length;n++)if(t[n].getAttribute("name")===e)return t[n];return""}if(window.localStorage&&localStorage.getItem("theme")){let e=localStorage.getItem("theme");e==="light"||e==="dark"?setTheme(e):setTheme(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")}else"auto"==="light"||"auto"==="dark"?(setTheme("auto"),saveTheme("auto")):(saveTheme("auto"),setTheme(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"));let metaColors={light:"#f8f8f8",dark:"#161b22"};getMeta("theme-color").content=metaColors[document.body.getAttribute("theme")],window.switchThemeEventSet=new Set</script><div id=back-to-top></div><div id=mask></div><header class="desktop print:!tw-hidden" id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="每日深度思考 | 技术、经济分析与深度思考"><span id=desktop-header-typeit class=typeit></span></a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>文章 </a><a class=menu-item href=/tags/>标签 </a><a class=menu-item href=/categories/>分类 </a><a class=menu-item href=/profile/ title=了解我的个人信息和专业背景>关于我 </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder=搜索... id=search-input-desktop>
<button class="search-button search-toggle" id=search-toggle-desktop title=搜索>
<svg class="icon" viewBox="0 0 512 512"><path d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</button>
<button class="search-button search-clear" id=search-clear-desktop title=清空>
<svg class="icon" viewBox="0 0 512 512"><path d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm121.6 313.1c4.7 4.7 4.7 12.3.0 17L338 377.6c-4.7 4.7-12.3 4.7-17 0L256 312l-65.1 65.6c-4.7 4.7-12.3 4.7-17 0L134.4 338c-4.7-4.7-4.7-12.3.0-17l65.6-65-65.6-65.1c-4.7-4.7-4.7-12.3.0-17l39.6-39.6c4.7-4.7 12.3-4.7 17 0l65 65.7 65.1-65.6c4.7-4.7 12.3-4.7 17 0l39.6 39.6c4.7 4.7 4.7 12.3.0 17L312 256l65.6 65.1z"/></svg>
</button>
<span class="search-button search-loading tw-animate-spin" id=search-loading-desktop><svg class="icon" viewBox="0 0 512 512"><path d="M304 48c0 26.51-21.49 48-48 48s-48-21.49-48-48 21.49-48 48-48 48 21.49 48 48zm-48 368c-26.51.0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zm208-208c-26.51.0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zM96 256c0-26.51-21.49-48-48-48S0 229.49.0 256s21.49 48 48 48 48-21.49 48-48zm12.922 99.078c-26.51.0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48c0-26.509-21.491-48-48-48zm294.156.0c-26.51.0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48c0-26.509-21.49-48-48-48zM108.922 60.922c-26.51.0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.491-48-48-48z"/></svg>
</span></span><button class="menu-item theme-switch" aria-label=切换主题>
<svg class="icon" viewBox="0 0 512 512"><path d="M8 256c0 136.966 111.033 248 248 248s248-111.034 248-248S392.966 8 256 8 8 119.033 8 256zm248 184V72c101.705.0 184 82.311 184 184 0 101.705-82.311 184-184 184z"/></svg></button></div></div></div></header><header class="mobile print:!tw-hidden" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="每日深度思考 | 技术、经济分析与深度思考"><span id=mobile-header-typeit class=typeit></span></a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索... id=search-input-mobile>
<button class="search-button search-toggle tw-h-10" id=search-toggle-mobile title=搜索>
<svg class="icon" viewBox="0 0 512 512"><path d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</button>
<button class="search-button search-clear tw-h-fit" id=search-clear-mobile title=清空>
<svg class="icon" viewBox="0 0 512 512"><path d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm121.6 313.1c4.7 4.7 4.7 12.3.0 17L338 377.6c-4.7 4.7-12.3 4.7-17 0L256 312l-65.1 65.6c-4.7 4.7-12.3 4.7-17 0L134.4 338c-4.7-4.7-4.7-12.3.0-17l65.6-65-65.6-65.1c-4.7-4.7-4.7-12.3.0-17l39.6-39.6c4.7-4.7 12.3-4.7 17 0l65 65.7 65.1-65.6c4.7-4.7 12.3-4.7 17 0l39.6 39.6c4.7 4.7 4.7 12.3.0 17L312 256l65.6 65.1z"/></svg>
</button>
<span class="search-button search-loading tw-animate-spin" id=search-loading-mobile><svg class="icon" viewBox="0 0 512 512"><path d="M304 48c0 26.51-21.49 48-48 48s-48-21.49-48-48 21.49-48 48-48 48 21.49 48 48zm-48 368c-26.51.0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zm208-208c-26.51.0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zM96 256c0-26.51-21.49-48-48-48S0 229.49.0 256s21.49 48 48 48 48-21.49 48-48zm12.922 99.078c-26.51.0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48c0-26.509-21.491-48-48-48zm294.156.0c-26.51.0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48c0-26.509-21.49-48-48-48zM108.922 60.922c-26.51.0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.491-48-48-48z"/></svg></span></div><button class=search-cancel id=search-cancel-mobile>
取消</button></div><a class=menu-item href=/posts/ title>文章</a><a class=menu-item href=/tags/ title>标签</a><a class=menu-item href=/categories/ title>分类</a><a class=menu-item href=/profile/ title=了解我的个人信息和专业背景>关于我</a><button class="menu-item theme-switch tw-w-full" aria-label=切换主题>
<svg class="icon" viewBox="0 0 512 512"><path d="M8 256c0 136.966 111.033 248 248 248s248-111.034 248-248S392.966 8 256 8 8 119.033 8 256zm248 184V72c101.705.0 184 82.311 184 184 0 101.705-82.311 184-184 184z"/></svg></button></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class="tw-mx-4 tw-flex-1"><dialog id=toc-dialog class="tw-max-w-full tw-w-full tw-max-h-full tw-h-full tw-ml-16"><div class="toc tw-mx-4 tw-max-w-full"><h2 class="tw-mx-0 tw-my-6 tw-uppercase tw-text-2xl">目录</h2><div class=toc-content><nav id=TableOfContents><ul><li><a href=#-一transformer-是什么>🧩 一、Transformer 是什么？</a></li><li><a href=#-二为什么要发明-transformer>🧠 二、为什么要发明 Transformer？</a></li><li><a href=#-三transformer-的核心结构简化版>⚙️ 三、Transformer 的核心结构（简化版）</a><ul><li><a href=#1-输入嵌入embedding>1️⃣ 输入嵌入（Embedding）</a></li><li><a href=#2-位置编码positional-encoding>2️⃣ 位置编码（Positional Encoding）</a></li><li><a href=#3-自注意力机制self-attention>3️⃣ 自注意力机制（Self-Attention）</a></li></ul></li><li><a href=#-四注意力机制深度解析>🔍 四、注意力机制深度解析</a><ul><li><a href=#-什么是注意力机制>💫 什么是注意力机制？</a></li><li><a href=#-注意力机制的数学原理>🧮 注意力机制的数学原理</a><ul><li><a href=#1-三要素querykeyvalue>1. 三要素：Query、Key、Value</a></li><li><a href=#2-注意力权重计算>2. 注意力权重计算</a></li></ul></li><li><a href=#-生动例子演示>🎪 生动例子演示</a><ul><li><a href=#例-1句子理解>例 1：句子理解</a></li><li><a href=#例-2多义词消歧>例 2：多义词消歧</a></li></ul></li><li><a href=#-多头注意力multi-head-attention>🚀 多头注意力（Multi-Head Attention）</a></li><li><a href=#-注意力机制的变体>🔗 注意力机制的变体</a></li><li><a href=#-注意力模式可视化>📊 注意力模式可视化</a></li><li><a href=#-注意力机制的优势>⚡ 注意力机制的优势</a></li><li><a href=#-注意力机制的局限性>🎯 注意力机制的局限性</a></li><li><a href=#-实际代码示例简化版>🧪 实际代码示例（简化版）</a></li><li><a href=#4-前馈神经网络feed-forward-network>4️⃣ 前馈神经网络（Feed-Forward Network）</a></li><li><a href=#5-层归一化layer-normalization--残差连接residual-connection>5️⃣ 层归一化（Layer Normalization） & 残差连接（Residual Connection）</a></li><li><a href=#6-编码器encoder--解码器decoder>6️⃣ 编码器（Encoder） & 解码器（Decoder）</a></li></ul></li><li><a href=#-五transformer-的运行流程以-gpt-为例>🔄 五、Transformer 的运行流程（以 GPT 为例）</a></li><li><a href=#-六为什么-transformer-如此强大>📈 六、为什么 Transformer 如此强大？</a></li><li><a href=#-七专业名词解释表>📘 七、专业名词解释表</a></li><li><a href=#-八一句话总结>✨ 八、一句话总结</a></li><li><a href=#-延伸阅读>📚 延伸阅读</a><ul><li><a href=#-ai-大模型系统教程系列>🔗 AI 大模型系统教程系列</a></li><li><a href=#-深入学习建议>🎯 深入学习建议</a></li></ul></li></ul></nav></div></div></dialog><script>document.getElementsByTagName("main")[0].setAttribute("autoTOC","false")</script><article class="page single print:!tw-w-full print:!tw-max-w-none print:!tw-m-0 print:!tw-p-0"><h1 class=single-title data-pagefind-meta=date:2025-11-05 data-pagefind-body>Transformer架构深度解析：注意力机制与AI大模型的核心技术</h1><div class=post-meta><div class=post-meta-line><span class=post-author><img class="tw-inline-block tw-max-h-4 tw-rounded-full tw-translate-y-[-2px] tw-mr-1" src=/static/avatar/angryCat.png alt="Finn avatar" height=16 width=16><a href=https://blog.baifan.site title=Author target=_blank rel="noopener noreferrer author" class=author>Finn</a>
</span>&nbsp;<span class=post-category>收录于 </span>&nbsp;<span class=post-category>类别 <a href=/categories/ai%E6%95%99%E7%A8%8B/><svg class="icon" viewBox="0 0 512 512"><path d="M464 128H272l-54.63-54.63c-6-6-14.14-9.37-22.63-9.37H48C21.49 64 0 85.49.0 112v288c0 26.51 21.49 48 48 48h416c26.51.0 48-21.49 48-48V176c0-26.51-21.49-48-48-48zm0 272H48V112h140.12l54.63 54.63c6 6 14.14 9.37 22.63 9.37H464v224z"/></svg>AI教程</a>&nbsp;<a href=/categories/%E6%8A%80%E6%9C%AF%E6%B7%B1%E5%BA%A6/><svg class="icon" viewBox="0 0 512 512"><path d="M464 128H272l-54.63-54.63c-6-6-14.14-9.37-22.63-9.37H48C21.49 64 0 85.49.0 112v288c0 26.51 21.49 48 48 48h416c26.51.0 48-21.49 48-48V176c0-26.51-21.49-48-48-48zm0 272H48V112h140.12l54.63 54.63c6 6 14.14 9.37 22.63 9.37H464v224z"/></svg>技术深度</a>&nbsp;<a href=/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/><svg class="icon" viewBox="0 0 512 512"><path d="M464 128H272l-54.63-54.63c-6-6-14.14-9.37-22.63-9.37H48C21.49 64 0 85.49.0 112v288c0 26.51 21.49 48 48 48h416c26.51.0 48-21.49 48-48V176c0-26.51-21.49-48-48-48zm0 272H48V112h140.12l54.63 54.63c6 6 14.14 9.37 22.63 9.37H464v224z"/></svg>人工智能</a></span>&nbsp;<span class=post-category>和</span>&nbsp;<span class=post-series>系列 <a href><svg class="icon" viewBox="0 0 512 512"><path d="M464 32H48C21.49 32 0 53.49.0 80v352c0 26.51 21.49 48 48 48h416c26.51.0 48-21.49 48-48V80c0-26.51-21.49-48-48-48zm-6 4e2H54a6 6 0 01-6-6V86a6 6 0 016-6h404a6 6 0 016 6v340a6 6 0 01-6 6zm-42-92v24c0 6.627-5.373 12-12 12H204c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h2e2c6.627.0 12 5.373 12 12zm0-96v24c0 6.627-5.373 12-12 12H204c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h2e2c6.627.0 12 5.373 12 12zm0-96v24c0 6.627-5.373 12-12 12H204c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h2e2c6.627.0 12 5.373 12 12zm-252 12c0 19.882-16.118 36-36 36s-36-16.118-36-36 16.118-36 36-36 36 16.118 36 36zm0 96c0 19.882-16.118 36-36 36s-36-16.118-36-36 16.118-36 36-36 36 16.118 36 36zm0 96c0 19.882-16.118 36-36 36s-36-16.118-36-36 16.118-36 36-36 36 16.118 36 36z"/></svg></a></span></div><div class=post-meta-line><svg class="icon" viewBox="0 0 448 512"><path d="M148 288h-40c-6.6.0-12-5.4-12-12v-40c0-6.6 5.4-12 12-12h40c6.6.0 12 5.4 12 12v40c0 6.6-5.4 12-12 12zm108-12v-40c0-6.6-5.4-12-12-12h-40c-6.6.0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6.0 12-5.4 12-12zm96 0v-40c0-6.6-5.4-12-12-12h-40c-6.6.0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6.0 12-5.4 12-12zm-96 96v-40c0-6.6-5.4-12-12-12h-40c-6.6.0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6.0 12-5.4 12-12zm-96 0v-40c0-6.6-5.4-12-12-12h-40c-6.6.0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6.0 12-5.4 12-12zm192 0v-40c0-6.6-5.4-12-12-12h-40c-6.6.0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6.0 12-5.4 12-12zm96-260v352c0 26.5-21.5 48-48 48H48c-26.5.0-48-21.5-48-48V112c0-26.5 21.5-48 48-48h48V12c0-6.6 5.4-12 12-12h40c6.6.0 12 5.4 12 12v52h128V12c0-6.6 5.4-12 12-12h40c6.6.0 12 5.4 12 12v52h48c26.5.0 48 21.5 48 48zm-48 346V160H48v298c0 3.3 2.7 6 6 6h340c3.3.0 6-2.7 6-6z"/></svg>&nbsp;<time datetime=2025-11-05>2025-11-05</time>&nbsp;<svg class="icon" viewBox="0 0 576 512"><path d="M402.3 344.9l32-32c5-5 13.7-1.5 13.7 5.7V464c0 26.5-21.5 48-48 48H48c-26.5.0-48-21.5-48-48V112c0-26.5 21.5-48 48-48h273.5c7.1.0 10.7 8.6 5.7 13.7l-32 32c-1.5 1.5-3.5 2.3-5.7 2.3H48v352h352V350.5c0-2.1.8-4.1 2.3-5.6zm156.6-201.8L296.3 405.7l-90.4 10c-26.2 2.9-48.5-19.2-45.6-45.6l10-90.4L432.9 17.1c22.9-22.9 59.9-22.9 82.7.0l43.2 43.2c22.9 22.9 22.9 60 .1 82.8zM460.1 174 402 115.9 216.2 301.8l-7.3 65.3 65.3-7.3L460.1 174zm64.8-79.7-43.2-43.2c-4.1-4.1-10.8-4.1-14.8.0L436 82l58.1 58.1 30.9-30.9c4-4.2 4-10.8-.1-14.9z"/></svg>&nbsp;<time datetime=2025-11-06>2025-11-06</time>&nbsp;<svg class="icon" viewBox="0 0 512 512"><path d="M497.9 142.1l-46.1 46.1c-4.7 4.7-12.3 4.7-17 0l-111-111c-4.7-4.7-4.7-12.3.0-17l46.1-46.1c18.7-18.7 49.1-18.7 67.9.0l60.1 60.1c18.8 18.7 18.8 49.1.0 67.9zM284.2 99.8 21.6 362.4.4 483.9c-2.9 16.4 11.4 30.6 27.8 27.8l121.5-21.3 262.6-262.6c4.7-4.7 4.7-12.3.0-17l-111-111c-4.8-4.7-12.4-4.7-17.1.0zM124.1 339.9c-5.5-5.5-5.5-14.3.0-19.8l154-154c5.5-5.5 14.3-5.5 19.8.0s5.5 14.3.0 19.8l-154 154c-5.5 5.5-14.3 5.5-19.8.0zM88 424h48v36.3l-64.5 11.3-31.1-31.1L51.7 376H88v48z"/></svg>&nbsp;约 3748 字&nbsp;
<svg class="icon" viewBox="0 0 512 512"><path d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm0 448c-110.5.0-2e2-89.5-2e2-2e2S145.5 56 256 56s2e2 89.5 2e2 2e2-89.5 2e2-2e2 2e2zm61.8-104.4-84.9-61.7c-3.1-2.3-4.9-5.9-4.9-9.7V116c0-6.6 5.4-12 12-12h32c6.6.0 12 5.4 12 12v141.7l66.8 48.6c5.4 3.9 6.5 11.4 2.6 16.8L334.6 349c-3.9 5.3-11.4 6.5-16.8 2.6z"/></svg>&nbsp;预计阅读 17 分钟&nbsp;</div></div><div class="details toc print:!tw-block" id=toc-static kept=true><div class="details-summary toc-title"><span>目录</span>
<span class=details-icon><svg class="icon" viewBox="0 0 256 512"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9.0l-22.6-22.6c-9.4-9.4-9.4-24.6.0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6.0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9.0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#-一transformer-是什么>🧩 一、Transformer 是什么？</a></li><li><a href=#-二为什么要发明-transformer>🧠 二、为什么要发明 Transformer？</a></li><li><a href=#-三transformer-的核心结构简化版>⚙️ 三、Transformer 的核心结构（简化版）</a><ul><li><a href=#1-输入嵌入embedding>1️⃣ 输入嵌入（Embedding）</a></li><li><a href=#2-位置编码positional-encoding>2️⃣ 位置编码（Positional Encoding）</a></li><li><a href=#3-自注意力机制self-attention>3️⃣ 自注意力机制（Self-Attention）</a></li></ul></li><li><a href=#-四注意力机制深度解析>🔍 四、注意力机制深度解析</a><ul><li><a href=#-什么是注意力机制>💫 什么是注意力机制？</a></li><li><a href=#-注意力机制的数学原理>🧮 注意力机制的数学原理</a><ul><li><a href=#1-三要素querykeyvalue>1. 三要素：Query、Key、Value</a></li><li><a href=#2-注意力权重计算>2. 注意力权重计算</a></li></ul></li><li><a href=#-生动例子演示>🎪 生动例子演示</a><ul><li><a href=#例-1句子理解>例 1：句子理解</a></li><li><a href=#例-2多义词消歧>例 2：多义词消歧</a></li></ul></li><li><a href=#-多头注意力multi-head-attention>🚀 多头注意力（Multi-Head Attention）</a></li><li><a href=#-注意力机制的变体>🔗 注意力机制的变体</a></li><li><a href=#-注意力模式可视化>📊 注意力模式可视化</a></li><li><a href=#-注意力机制的优势>⚡ 注意力机制的优势</a></li><li><a href=#-注意力机制的局限性>🎯 注意力机制的局限性</a></li><li><a href=#-实际代码示例简化版>🧪 实际代码示例（简化版）</a></li><li><a href=#4-前馈神经网络feed-forward-network>4️⃣ 前馈神经网络（Feed-Forward Network）</a></li><li><a href=#5-层归一化layer-normalization--残差连接residual-connection>5️⃣ 层归一化（Layer Normalization） & 残差连接（Residual Connection）</a></li><li><a href=#6-编码器encoder--解码器decoder>6️⃣ 编码器（Encoder） & 解码器（Decoder）</a></li></ul></li><li><a href=#-五transformer-的运行流程以-gpt-为例>🔄 五、Transformer 的运行流程（以 GPT 为例）</a></li><li><a href=#-六为什么-transformer-如此强大>📈 六、为什么 Transformer 如此强大？</a></li><li><a href=#-七专业名词解释表>📘 七、专业名词解释表</a></li><li><a href=#-八一句话总结>✨ 八、一句话总结</a></li><li><a href=#-延伸阅读>📚 延伸阅读</a><ul><li><a href=#-ai-大模型系统教程系列>🔗 AI 大模型系统教程系列</a></li><li><a href=#-深入学习建议>🎯 深入学习建议</a></li></ul></li></ul></nav></div></div><div class=content id=content data-pagefind-body><h1 id=ai-教程---transformer class=headerLink><a href=#ai-%e6%95%99%e7%a8%8b---transformer class=header-mark></a>AI 教程 - Transformer</h1><h2 id=-一transformer-是什么 class=headerLink><a href=#-%e4%b8%80transformer-%e6%98%af%e4%bb%80%e4%b9%88 class=header-mark></a>🧩 一、Transformer 是什么？</h2><blockquote><p><strong>Transformer 是一种深度学习架构，用来处理序列（例如文字、语音、代码等）信息。</strong></p></blockquote><p>它最早由 Google 在 2017 年的论文《Attention Is All You Need（注意力机制就是全部）》中提出。</p><p>这篇论文奠定了今天几乎所有大语言模型的基础。GPT、BERT、Claude、Gemini、通义千问、文心一言——统统基于 Transformer。</p><hr><h2 id=-二为什么要发明-transformer class=headerLink><a href=#-%e4%ba%8c%e4%b8%ba%e4%bb%80%e4%b9%88%e8%a6%81%e5%8f%91%e6%98%8e-transformer class=header-mark></a>🧠 二、为什么要发明 Transformer？</h2><p>在 Transformer 出现之前，主流的序列模型是：</p><div class=table-wrapper><table><thead><tr><th style=text-align:>模型类型</th><th style=text-align:>英文名称</th><th style=text-align:>主要问题</th></tr></thead><tbody><tr><td style=text-align:>循环神经网络 (RNN)</td><td style=text-align:>Recurrent Neural Network</td><td style=text-align:>逐字处理，速度慢</td></tr><tr><td style=text-align:>长短期记忆网络 (LSTM)</td><td style=text-align:>Long Short-Term Memory</td><td style=text-align:>长文本记忆能力差</td></tr><tr><td style=text-align:>卷积神经网络 (CNN)</td><td style=text-align:>Convolutional Neural Network</td><td style=text-align:>不擅长顺序理解</td></tr></tbody></table></div><p>这些模型要么太慢，要么不能理解长距离关系。</p><p>Transformer 的突破在于引入了：</p><blockquote><p>🌟 <strong>自注意力机制（Self-Attention）</strong>，让模型一次性看到整段文字，并学会"关注重点"。</p></blockquote><hr><h2 id=-三transformer-的核心结构简化版 class=headerLink><a href=#-%e4%b8%89transformer-%e7%9a%84%e6%a0%b8%e5%bf%83%e7%bb%93%e6%9e%84%e7%ae%80%e5%8c%96%e7%89%88 class=header-mark></a>⚙️ 三、Transformer 的核心结构（简化版）</h2><p>可以想象 Transformer 是一个<strong>巨大的堆叠积木塔</strong>，每一层都有几个关键模块：</p><h3 id=1-输入嵌入embedding class=headerLink><a href=#1-%e8%be%93%e5%85%a5%e5%b5%8c%e5%85%a5embedding class=header-mark></a>1️⃣ 输入嵌入（Embedding）</h3><p>把文字（token）转换成向量形式，例如：&ldquo;我喜欢苹果&rdquo; → 向量矩阵 <code>[0.4, -0.1, 0.8, …]</code></p><h3 id=2-位置编码positional-encoding class=headerLink><a href=#2-%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81positional-encoding class=header-mark></a>2️⃣ 位置编码（Positional Encoding）</h3><p>因为 Transformer 同时读入整段话（不像 RNN 一次一个），它必须知道"顺序"。因此给每个词加上"位置信号"，比如第 1 个、第 2 个、第 3 个。</p><h3 id=3-自注意力机制self-attention class=headerLink><a href=#3-%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6self-attention class=header-mark></a>3️⃣ 自注意力机制（Self-Attention）</h3><p>这是 Transformer 的灵魂 ✨</p><p>它让模型可以<strong>自动决定该关注哪些词</strong>。</p><p>比如：</p><blockquote><p>&ldquo;我去银行存钱&rdquo;
&ldquo;我在河边的银行钓鱼&rdquo;</p></blockquote><p>模型会通过"注意力"判断：</p><ul><li>第一句中"银行"要关注"钱"；</li><li>第二句中"银行"要关注"河"。</li></ul><p>📌 <strong>技术上</strong>：每个词都会计算出三个向量：</p><ul><li>Query（查询）</li><li>Key（键）</li><li>Value（值）</li></ul><p>然后用这些向量计算出每个词对其他词的"相关程度（权重）"，最终形成一个加权求和的"上下文理解"。</p><hr><h2 id=-四注意力机制深度解析 class=headerLink><a href=#-%e5%9b%9b%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90 class=header-mark></a>🔍 四、注意力机制深度解析</h2><h3 id=-什么是注意力机制 class=headerLink><a href=#-%e4%bb%80%e4%b9%88%e6%98%af%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6 class=header-mark></a>💫 什么是注意力机制？</h3><p>**注意力机制（Attention Mechanism）**是人类认知过程的数学模拟。就像我们在阅读时会自然地重点关注某些关键词一样，注意力机制让模型能够"聚焦"于输入序列中的重要部分。</p><blockquote><p>🎯 <strong>核心思想</strong>：不是所有输入信息都同等重要，模型应该学会分配不同的"注意力权重"。</p></blockquote><h3 id=-注意力机制的数学原理 class=headerLink><a href=#-%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%e7%9a%84%e6%95%b0%e5%ad%a6%e5%8e%9f%e7%90%86 class=header-mark></a>🧮 注意力机制的数学原理</h3><h4 id=1-三要素querykeyvalue class=headerLink><a href=#1-%e4%b8%89%e8%a6%81%e7%b4%a0querykeyvalue class=header-mark></a>1. 三要素：Query、Key、Value</h4><p>每个词都生成三个向量：</p><div class=table-wrapper><table><thead><tr><th style=text-align:>向量</th><th style=text-align:>符号</th><th style=text-align:>作用</th><th style=text-align:>比喻</th></tr></thead><tbody><tr><td style=text-align:><strong>Query</strong></td><td style=text-align:>Q</td><td style=text-align:>&ldquo;我要找什么&rdquo;</td><td style=text-align:>🔍 搜索时的查询词</td></tr><tr><td style=text-align:><strong>Key</strong></td><td style=text-align:>K</td><td style=text-align:>&ldquo;我能提供什么&rdquo;</td><td style=text-align:>🏷️ 文章的标签</td></tr><tr><td style=text-align:><strong>Value</strong></td><td style=text-align:>V</td><td style=text-align:>&ldquo;我的实际内容&rdquo;</td><td style=text-align:>📄 文章的正文</td></tr></tbody></table></div><h4 id=2-注意力权重计算 class=headerLink><a href=#2-%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9d%83%e9%87%8d%e8%ae%a1%e7%ae%97 class=header-mark></a>2. 注意力权重计算</h4><p><strong>公式：</strong> <code>Attention(Q,K,V) = softmax(QK^T/√d_k)V</code></p><p><strong>步骤分解：</strong></p><ol><li><strong>相似度计算</strong>：<code>Q × K^T</code> - Query 与每个 Key 的匹配度</li><li><strong>缩放</strong>：<code>÷ √d_k</code> - 防止梯度消失（d_k 是 Key 向量的维度）</li><li><strong>归一化</strong>：<code>softmax()</code> - 转换为概率分布（权重和为 1）</li><li><strong>加权求和</strong>：<code>× V</code> - 用权重对 Value 进行加权平均</li></ol><h3 id=-生动例子演示 class=headerLink><a href=#-%e7%94%9f%e5%8a%a8%e4%be%8b%e5%ad%90%e6%bc%94%e7%a4%ba class=header-mark></a>🎪 生动例子演示</h3><h4 id=例-1句子理解 class=headerLink><a href=#%e4%be%8b-1%e5%8f%a5%e5%ad%90%e7%90%86%e8%a7%a3 class=header-mark></a>例 1：句子理解</h4><p><strong>输入句子</strong>：&ldquo;小明喜欢苹果，因为它们很甜&rdquo;</p><p><strong>注意力权重可视化</strong>：</p><div class=table-wrapper><table><thead><tr><th style=text-align:>关注词</th><th style=text-align:>小明</th><th style=text-align:>喜欢</th><th style=text-align:>苹果</th><th style=text-align:>因为</th><th style=text-align:>它们</th><th style=text-align:>很</th><th style=text-align:>甜</th></tr></thead><tbody><tr><td style=text-align:><strong>苹果</strong></td><td style=text-align:>0.05</td><td style=text-align:>0.15</td><td style=text-align:>0.60</td><td style=text-align:>0.10</td><td style=text-align:>0.05</td><td style=text-align:>0.03</td><td style=text-align:>0.02</td></tr><tr><td style=text-align:><strong>它们</strong></td><td style=text-align:>0.02</td><td style=text-align:>0.08</td><td style=text-align:>0.45</td><td style=text-align:>0.20</td><td style=text-align:>0.15</td><td style=text-align:>0.07</td><td style=text-align:>0.03</td></tr><tr><td style=text-align:><strong>甜</strong></td><td style=text-align:>0.01</td><td style=text-align:>0.05</td><td style=text-align:>0.20</td><td style=text-align:>0.25</td><td style=text-align:>0.15</td><td style=text-align:>0.25</td><td style=text-align:>0.09</td></tr></tbody></table></div><p><strong>解释</strong>：</p><ul><li>&ldquo;苹果"主要关注自身（0.60），也关注"喜欢&rdquo;（0.15）</li><li>&ldquo;它们"重点关注"苹果&rdquo;（0.45），理解指代关系</li><li>&ldquo;甜"与"很"形成副词修饰关系</li></ul><h4 id=例-2多义词消歧 class=headerLink><a href=#%e4%be%8b-2%e5%a4%9a%e4%b9%89%e8%af%8d%e6%b6%88%e6%ad%a7 class=header-mark></a>例 2：多义词消歧</h4><p><strong>句子 1</strong>：&ldquo;我去<strong>银行</strong>取钱&rdquo;
<strong>句子 2</strong>：&ldquo;河边的<strong>银行</strong>柳树摇曳&rdquo;</p><div class=table-wrapper><table><thead><tr><th style=text-align:>句子</th><th style=text-align:>钱(0.42)</th><th style=text-align:>取(0.23)</th><th style=text-align:>河(0.08)</th><th style=text-align:>边(0.05)</th><th style=text-align:>柳(0.02)</th></tr></thead><tbody><tr><td style=text-align:><strong>句子 1</strong></td><td style=text-align:>🏦 <strong>金融机构</strong></td><td style=text-align:></td><td style=text-align:></td><td style=text-align:></td><td style=text-align:></td></tr><tr><td style=text-align:><strong>句子 2</strong></td><td style=text-align:></td><td style=text-align:></td><td style=text-align:>🌊 <strong>河岸</strong></td><td style=text-align:></td><td style=text-align:>🌳</td></tr></tbody></table></div><p><strong>结果</strong>：注意力权重帮助模型正确理解"银行"的不同含义。</p><h3 id=-多头注意力multi-head-attention class=headerLink><a href=#-%e5%a4%9a%e5%a4%b4%e6%b3%a8%e6%84%8f%e5%8a%9bmulti-head-attention class=header-mark></a>🚀 多头注意力（Multi-Head Attention）</h3><p><strong>为什么需要多头？</strong></p><blockquote><p>单个注意力机制只能捕捉一种关系，多头注意力让模型同时关注多种不同类型的关系。</p></blockquote><p><strong>工作原理</strong>：</p><div class="code-block highlight is-open show-line-numbers tw-group tw-my-2"><div class="tw-flex
tw-flex-row
tw-flex-1
tw-justify-between
tw-w-full tw-bg-bgColor-secondary"><button class="code-block-button
tw-mx-2
tw-flex
tw-flex-row
tw-flex-1" aria-hidden=true><div class="group-[.is-open]:tw-rotate-90 tw-transition-[transform] tw-duration-500 tw-ease-in-out print:!tw-hidden tw-w-min tw-h-min tw-my-1 tw-mx-1"><svg class="icon" viewBox="0 0 320 512"><path d="M285.476 272.971 91.132 467.314c-9.373 9.373-24.569 9.373-33.941.0l-22.667-22.667c-9.357-9.357-9.375-24.522-.04-33.901L188.505 256 34.484 101.255c-9.335-9.379-9.317-24.544.04-33.901l22.667-22.667c9.373-9.373 24.569-9.373 33.941.0L285.475 239.03c9.373 9.372 9.373 24.568.001 33.941z"/></svg></div><p class="tw-select-none !tw-my-1">text</p></button><div class=tw-flex><button class="line-number-button
tw-mx-2
tw-hidden
group-[.is-open]:tw-block
group-[.show-line-numbers]:tw-text-fgColor-link
print:!tw-hidden" title="Toggle line numbers"><svg class="icon" viewBox="0 0 512 512"><path d="M61.77 401l17.5-20.15a19.92 19.92.0 005.07-14.19v-3.31C84.34 356 80.5 352 73 352H16a8 8 0 00-8 8v16a8 8 0 008 8h22.83a157.41 157.41.0 00-11 12.31l-5.61 7c-4 5.07-5.25 10.13-2.8 14.88l1.05 1.93c3 5.76 6.29 7.88 12.25 7.88h4.73c10.33.0 15.94 2.44 15.94 9.09.0 4.72-4.2 8.22-14.36 8.22a41.54 41.54.0 01-15.47-3.12c-6.49-3.88-11.74-3.5-15.6 3.12l-5.59 9.31c-3.72 6.13-3.19 11.72 2.63 15.94 7.71 4.69 20.38 9.44 37 9.44 34.16.0 48.5-22.75 48.5-44.12-.03-14.38-9.12-29.76-28.73-34.88zM496 224H176a16 16 0 00-16 16v32a16 16 0 0016 16h320a16 16 0 0016-16v-32a16 16 0 00-16-16zm0-160H176a16 16 0 00-16 16v32a16 16 0 0016 16h320a16 16 0 0016-16V80a16 16 0 00-16-16zm0 320H176a16 16 0 00-16 16v32a16 16 0 0016 16h320a16 16 0 0016-16v-32a16 16 0 00-16-16zM16 160h64a8 8 0 008-8v-16a8 8 0 00-8-8H64V40a8 8 0 00-8-8H32a8 8 0 00-7.14 4.42l-8 16A8 8 0 0024 64h8v64H16a8 8 0 00-8 8v16a8 8 0 008 8zm-3.91 160H80a8 8 0 008-8v-16a8 8 0 00-8-8H41.32c3.29-10.29 48.34-18.68 48.34-56.44.0-29.06-25-39.56-44.47-39.56-21.36.0-33.8 10-40.46 18.75-4.37 5.59-3 10.84 2.8 15.37l8.58 6.88c5.61 4.56 11 2.47 16.12-2.44a13.44 13.44.0 019.46-3.84c3.33.0 9.28 1.56 9.28 8.75C51 248.19.0 257.31.0 304.59v4C0 316 5.08 320 12.09 320z"/></svg></button>
<button class="wrap-code-button
tw-select-none
tw-mx-2
tw-hidden
group-[.is-open]:tw-block
group-[.is-wrap]:tw-text-fgColor-link
print:!tw-hidden" title="Toggle code wrap"><svg class="icon" viewBox="0 0 448 512"><path d="M16 132h416c8.837.0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163.0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837.0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837.0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837.0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837.0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"/></svg></button>
<button class="copy-code-button
tw-select-none
tw-mx-2
tw-hidden
group-[.is-open]:tw-block
hover:tw-text-fgColor-link
print:!tw-hidden" title="Copy code">
<span class="copy-icon tw-block"><svg class="icon" viewBox="0 0 448 512"><path d="M433.941 65.941l-51.882-51.882A48 48 0 00348.118.0H176c-26.51.0-48 21.49-48 48v48H48c-26.51.0-48 21.49-48 48v320c0 26.51 21.49 48 48 48h224c26.51.0 48-21.49 48-48v-48h80c26.51.0 48-21.49 48-48V99.882a48 48 0 00-14.059-33.941zM266 464H54a6 6 0 01-6-6V150a6 6 0 016-6h74v224c0 26.51 21.49 48 48 48h96v42a6 6 0 01-6 6zm128-96H182a6 6 0 01-6-6V54a6 6 0 016-6h106v88c0 13.255 10.745 24 24 24h88v202a6 6 0 01-6 6zm6-256h-64V48h9.632c1.591.0 3.117.632 4.243 1.757l48.368 48.368a6 6 0 011.757 4.243V112z"/></svg></span>
<span class="check-icon tw-hidden"><svg class="icon" viewBox="0 0 512 512"><path d="M173.898 439.404l-166.4-166.4c-9.997-9.997-9.997-26.206.0-36.204l36.203-36.204c9.997-9.998 26.207-9.998 36.204.0L192 312.69 432.095 72.596c9.997-9.997 26.207-9.997 36.204.0l36.203 36.204c9.997 9.997 9.997 26.206.0 36.204l-294.4 294.401c-9.998 9.997-26.207 9.997-36.204-.001z"/></svg></span>
</button>
<button class="tw-select-none
tw-mx-2
tw-block
group-[.is-open]:tw-hidden
print:!tw-hidden" disabled aria-hidden=true><svg class="icon" viewBox="0 0 512 512"><path d="M328 256c0 39.8-32.2 72-72 72s-72-32.2-72-72 32.2-72 72-72 72 32.2 72 72zm104-72c-39.8.0-72 32.2-72 72s32.2 72 72 72 72-32.2 72-72-32.2-72-72-72zm-352 0c-39.8.0-72 32.2-72 72s32.2 72 72 72 72-32.2 72-72-32.2-72-72-72z"/></svg></button></div></div><pre style=counter-reset:codeblock class="tw-block tw-m-0 tw-p-0"><code id=codeblock-id-1 class="chroma
!tw-block
tw-p-0
tw-m-0
tw-transition-[max-height]
tw-duration-500
tw-ease-in-out
group-[.is-closed]:!tw-max-h-0
group-[.is-wrap]:tw-text-wrap
tw-overflow-y-hidden
tw-overflow-x-auto
tw-scrollbar-thin"><span class=line><span class=cl>输入 → 拆分成8个头 → 并行计算8种注意力 → 合并结果</span></span></code></pre></div><p><strong>实际例子</strong>：&ldquo;张三告诉李四，他明天不来开会&rdquo;</p><div class=table-wrapper><table><thead><tr><th style=text-align:>注意力头</th><th style=text-align:>关注重点</th><th style=text-align:>发现的关系</th></tr></thead><tbody><tr><td style=text-align:><strong>头 1</strong></td><td style=text-align:>主谓关系</td><td style=text-align:>张三 → 告诉</td></tr><tr><td style=text-align:><strong>头 2</strong></td><td style=text-align:>宾语关系</td><td style=text-align:>告诉 → 李四</td></tr><tr><td style=text-align:><strong>头 3</strong></td><td style=text-align:>从句关系</td><td style=text-align:>告诉 → 不来</td></tr><tr><td style=text-align:><strong>头 4</strong></td><td style=text-align:>代词指代</td><td style=text-align:>他 → 张三</td></tr><tr><td style=text-align:><strong>头 5</strong></td><td style=text-align:>时间关系</td><td style=text-align:>明天 → 不来</td></tr><tr><td style=text-align:><strong>头 6</strong></td><td style=text-align:>地点关系</td><td style=text-align:>开会 →（隐含地点）</td></tr><tr><td style=text-align:><strong>头 7</strong></td><td style=text-align:>否定关系</td><td style=text-align:>不 → 来</td></tr><tr><td style=text-align:><strong>头 8</strong></td><td style=text-align:>未来时态</td><td style=text-align:>明天 →（未来）</td></tr></tbody></table></div><p><strong>数学表示</strong>：
<code>MultiHead(Q,K,V) = Concat(head₁,head₂,...,headₕ)W^O</code></p><p>其中 <code>headᵢ = Attention(QWᵢ^Q, KWᵢ^K, VWᵢ^V)</code></p><h3 id=-注意力机制的变体 class=headerLink><a href=#-%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%e7%9a%84%e5%8f%98%e4%bd%93 class=header-mark></a>🔗 注意力机制的变体</h3><div class=table-wrapper><table><thead><tr><th style=text-align:>变体</th><th style=text-align:>特点</th><th style=text-align:>应用场景</th></tr></thead><tbody><tr><td style=text-align:><strong>Self-Attention</strong></td><td style=text-align:>输入=输出，理解内部关系</td><td style=text-align:>BERT, GPT 的编码器</td></tr><tr><td style=text-align:><strong>Cross-Attention</strong></td><td style=text-align:>不同序列间的注意力</td><td style=text-align:>翻译、图文匹配</td></tr><tr><td style=text-align:><strong>Causal Attention</strong></td><td style=text-align:>只能关注前面内容</td><td style=text-align:>GPT 的解码器</td></tr><tr><td style=text-align:><strong>Sparse Attention</strong></td><td style=text-align:>减少计算复杂度</td><td style=text-align:>Longformer, BigBird</td></tr><tr><td style=text-align:><strong>Local Attention</strong></td><td style=text-align:>只关注局部窗口</td><td style=text-align:>Convolutional variants</td></tr></tbody></table></div><h3 id=-注意力模式可视化 class=headerLink><a href=#-%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%a8%a1%e5%bc%8f%e5%8f%af%e8%a7%86%e5%8c%96 class=header-mark></a>📊 注意力模式可视化</h3><p><strong>不同任务中的注意力模式</strong>：</p><ol><li><strong>语法分析</strong>：</li></ol><div class="code-block highlight is-open show-line-numbers tw-group tw-my-2"><div class="tw-flex
tw-flex-row
tw-flex-1
tw-justify-between
tw-w-full tw-bg-bgColor-secondary"><button class="code-block-button
tw-mx-2
tw-flex
tw-flex-row
tw-flex-1" aria-hidden=true><div class="group-[.is-open]:tw-rotate-90 tw-transition-[transform] tw-duration-500 tw-ease-in-out print:!tw-hidden tw-w-min tw-h-min tw-my-1 tw-mx-1"><svg class="icon" viewBox="0 0 320 512"><path d="M285.476 272.971 91.132 467.314c-9.373 9.373-24.569 9.373-33.941.0l-22.667-22.667c-9.357-9.357-9.375-24.522-.04-33.901L188.505 256 34.484 101.255c-9.335-9.379-9.317-24.544.04-33.901l22.667-22.667c9.373-9.373 24.569-9.373 33.941.0L285.475 239.03c9.373 9.372 9.373 24.568.001 33.941z"/></svg></div><p class="tw-select-none !tw-my-1">text</p></button><div class=tw-flex><button class="line-number-button
tw-mx-2
tw-hidden
group-[.is-open]:tw-block
group-[.show-line-numbers]:tw-text-fgColor-link
print:!tw-hidden" title="Toggle line numbers"><svg class="icon" viewBox="0 0 512 512"><path d="M61.77 401l17.5-20.15a19.92 19.92.0 005.07-14.19v-3.31C84.34 356 80.5 352 73 352H16a8 8 0 00-8 8v16a8 8 0 008 8h22.83a157.41 157.41.0 00-11 12.31l-5.61 7c-4 5.07-5.25 10.13-2.8 14.88l1.05 1.93c3 5.76 6.29 7.88 12.25 7.88h4.73c10.33.0 15.94 2.44 15.94 9.09.0 4.72-4.2 8.22-14.36 8.22a41.54 41.54.0 01-15.47-3.12c-6.49-3.88-11.74-3.5-15.6 3.12l-5.59 9.31c-3.72 6.13-3.19 11.72 2.63 15.94 7.71 4.69 20.38 9.44 37 9.44 34.16.0 48.5-22.75 48.5-44.12-.03-14.38-9.12-29.76-28.73-34.88zM496 224H176a16 16 0 00-16 16v32a16 16 0 0016 16h320a16 16 0 0016-16v-32a16 16 0 00-16-16zm0-160H176a16 16 0 00-16 16v32a16 16 0 0016 16h320a16 16 0 0016-16V80a16 16 0 00-16-16zm0 320H176a16 16 0 00-16 16v32a16 16 0 0016 16h320a16 16 0 0016-16v-32a16 16 0 00-16-16zM16 160h64a8 8 0 008-8v-16a8 8 0 00-8-8H64V40a8 8 0 00-8-8H32a8 8 0 00-7.14 4.42l-8 16A8 8 0 0024 64h8v64H16a8 8 0 00-8 8v16a8 8 0 008 8zm-3.91 160H80a8 8 0 008-8v-16a8 8 0 00-8-8H41.32c3.29-10.29 48.34-18.68 48.34-56.44.0-29.06-25-39.56-44.47-39.56-21.36.0-33.8 10-40.46 18.75-4.37 5.59-3 10.84 2.8 15.37l8.58 6.88c5.61 4.56 11 2.47 16.12-2.44a13.44 13.44.0 019.46-3.84c3.33.0 9.28 1.56 9.28 8.75C51 248.19.0 257.31.0 304.59v4C0 316 5.08 320 12.09 320z"/></svg></button>
<button class="wrap-code-button
tw-select-none
tw-mx-2
tw-hidden
group-[.is-open]:tw-block
group-[.is-wrap]:tw-text-fgColor-link
print:!tw-hidden" title="Toggle code wrap"><svg class="icon" viewBox="0 0 448 512"><path d="M16 132h416c8.837.0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163.0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837.0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837.0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837.0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837.0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"/></svg></button>
<button class="copy-code-button
tw-select-none
tw-mx-2
tw-hidden
group-[.is-open]:tw-block
hover:tw-text-fgColor-link
print:!tw-hidden" title="Copy code">
<span class="copy-icon tw-block"><svg class="icon" viewBox="0 0 448 512"><path d="M433.941 65.941l-51.882-51.882A48 48 0 00348.118.0H176c-26.51.0-48 21.49-48 48v48H48c-26.51.0-48 21.49-48 48v320c0 26.51 21.49 48 48 48h224c26.51.0 48-21.49 48-48v-48h80c26.51.0 48-21.49 48-48V99.882a48 48 0 00-14.059-33.941zM266 464H54a6 6 0 01-6-6V150a6 6 0 016-6h74v224c0 26.51 21.49 48 48 48h96v42a6 6 0 01-6 6zm128-96H182a6 6 0 01-6-6V54a6 6 0 016-6h106v88c0 13.255 10.745 24 24 24h88v202a6 6 0 01-6 6zm6-256h-64V48h9.632c1.591.0 3.117.632 4.243 1.757l48.368 48.368a6 6 0 011.757 4.243V112z"/></svg></span>
<span class="check-icon tw-hidden"><svg class="icon" viewBox="0 0 512 512"><path d="M173.898 439.404l-166.4-166.4c-9.997-9.997-9.997-26.206.0-36.204l36.203-36.204c9.997-9.998 26.207-9.998 36.204.0L192 312.69 432.095 72.596c9.997-9.997 26.207-9.997 36.204.0l36.203 36.204c9.997 9.997 9.997 26.206.0 36.204l-294.4 294.401c-9.998 9.997-26.207 9.997-36.204-.001z"/></svg></span>
</button>
<button class="tw-select-none
tw-mx-2
tw-block
group-[.is-open]:tw-hidden
print:!tw-hidden" disabled aria-hidden=true><svg class="icon" viewBox="0 0 512 512"><path d="M328 256c0 39.8-32.2 72-72 72s-72-32.2-72-72 32.2-72 72-72 72 32.2 72 72zm104-72c-39.8.0-72 32.2-72 72s32.2 72 72 72 72-32.2 72-72-32.2-72-72-72zm-352 0c-39.8.0-72 32.2-72 72s32.2 72 72 72 72-32.2 72-72-32.2-72-72-72z"/></svg></button></div></div><pre style=counter-reset:codeblock class="tw-block tw-m-0 tw-p-0"><code id=codeblock-id-2 class="chroma
!tw-block
tw-p-0
tw-m-0
tw-transition-[max-height]
tw-duration-500
tw-ease-in-out
group-[.is-closed]:!tw-max-h-0
group-[.is-wrap]:tw-text-wrap
tw-overflow-y-hidden
tw-overflow-x-auto
tw-scrollbar-thin"><span class=line><span class=cl>The cat sat on the mat
</span></span><span class=line><span class=cl> ↓  ↓   ↓  ↓  ↓  ↓
</span></span><span class=line><span class=cl>主语 谓语 介词 冠词 名词</span></span></code></pre></div><ol start=2><li><strong>指代消解</strong>：</li></ol><div class="code-block highlight is-open show-line-numbers tw-group tw-my-2"><div class="tw-flex
tw-flex-row
tw-flex-1
tw-justify-between
tw-w-full tw-bg-bgColor-secondary"><button class="code-block-button
tw-mx-2
tw-flex
tw-flex-row
tw-flex-1" aria-hidden=true><div class="group-[.is-open]:tw-rotate-90 tw-transition-[transform] tw-duration-500 tw-ease-in-out print:!tw-hidden tw-w-min tw-h-min tw-my-1 tw-mx-1"><svg class="icon" viewBox="0 0 320 512"><path d="M285.476 272.971 91.132 467.314c-9.373 9.373-24.569 9.373-33.941.0l-22.667-22.667c-9.357-9.357-9.375-24.522-.04-33.901L188.505 256 34.484 101.255c-9.335-9.379-9.317-24.544.04-33.901l22.667-22.667c9.373-9.373 24.569-9.373 33.941.0L285.475 239.03c9.373 9.372 9.373 24.568.001 33.941z"/></svg></div><p class="tw-select-none !tw-my-1">text</p></button><div class=tw-flex><button class="line-number-button
tw-mx-2
tw-hidden
group-[.is-open]:tw-block
group-[.show-line-numbers]:tw-text-fgColor-link
print:!tw-hidden" title="Toggle line numbers"><svg class="icon" viewBox="0 0 512 512"><path d="M61.77 401l17.5-20.15a19.92 19.92.0 005.07-14.19v-3.31C84.34 356 80.5 352 73 352H16a8 8 0 00-8 8v16a8 8 0 008 8h22.83a157.41 157.41.0 00-11 12.31l-5.61 7c-4 5.07-5.25 10.13-2.8 14.88l1.05 1.93c3 5.76 6.29 7.88 12.25 7.88h4.73c10.33.0 15.94 2.44 15.94 9.09.0 4.72-4.2 8.22-14.36 8.22a41.54 41.54.0 01-15.47-3.12c-6.49-3.88-11.74-3.5-15.6 3.12l-5.59 9.31c-3.72 6.13-3.19 11.72 2.63 15.94 7.71 4.69 20.38 9.44 37 9.44 34.16.0 48.5-22.75 48.5-44.12-.03-14.38-9.12-29.76-28.73-34.88zM496 224H176a16 16 0 00-16 16v32a16 16 0 0016 16h320a16 16 0 0016-16v-32a16 16 0 00-16-16zm0-160H176a16 16 0 00-16 16v32a16 16 0 0016 16h320a16 16 0 0016-16V80a16 16 0 00-16-16zm0 320H176a16 16 0 00-16 16v32a16 16 0 0016 16h320a16 16 0 0016-16v-32a16 16 0 00-16-16zM16 160h64a8 8 0 008-8v-16a8 8 0 00-8-8H64V40a8 8 0 00-8-8H32a8 8 0 00-7.14 4.42l-8 16A8 8 0 0024 64h8v64H16a8 8 0 00-8 8v16a8 8 0 008 8zm-3.91 160H80a8 8 0 008-8v-16a8 8 0 00-8-8H41.32c3.29-10.29 48.34-18.68 48.34-56.44.0-29.06-25-39.56-44.47-39.56-21.36.0-33.8 10-40.46 18.75-4.37 5.59-3 10.84 2.8 15.37l8.58 6.88c5.61 4.56 11 2.47 16.12-2.44a13.44 13.44.0 019.46-3.84c3.33.0 9.28 1.56 9.28 8.75C51 248.19.0 257.31.0 304.59v4C0 316 5.08 320 12.09 320z"/></svg></button>
<button class="wrap-code-button
tw-select-none
tw-mx-2
tw-hidden
group-[.is-open]:tw-block
group-[.is-wrap]:tw-text-fgColor-link
print:!tw-hidden" title="Toggle code wrap"><svg class="icon" viewBox="0 0 448 512"><path d="M16 132h416c8.837.0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163.0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837.0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837.0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837.0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837.0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"/></svg></button>
<button class="copy-code-button
tw-select-none
tw-mx-2
tw-hidden
group-[.is-open]:tw-block
hover:tw-text-fgColor-link
print:!tw-hidden" title="Copy code">
<span class="copy-icon tw-block"><svg class="icon" viewBox="0 0 448 512"><path d="M433.941 65.941l-51.882-51.882A48 48 0 00348.118.0H176c-26.51.0-48 21.49-48 48v48H48c-26.51.0-48 21.49-48 48v320c0 26.51 21.49 48 48 48h224c26.51.0 48-21.49 48-48v-48h80c26.51.0 48-21.49 48-48V99.882a48 48 0 00-14.059-33.941zM266 464H54a6 6 0 01-6-6V150a6 6 0 016-6h74v224c0 26.51 21.49 48 48 48h96v42a6 6 0 01-6 6zm128-96H182a6 6 0 01-6-6V54a6 6 0 016-6h106v88c0 13.255 10.745 24 24 24h88v202a6 6 0 01-6 6zm6-256h-64V48h9.632c1.591.0 3.117.632 4.243 1.757l48.368 48.368a6 6 0 011.757 4.243V112z"/></svg></span>
<span class="check-icon tw-hidden"><svg class="icon" viewBox="0 0 512 512"><path d="M173.898 439.404l-166.4-166.4c-9.997-9.997-9.997-26.206.0-36.204l36.203-36.204c9.997-9.998 26.207-9.998 36.204.0L192 312.69 432.095 72.596c9.997-9.997 26.207-9.997 36.204.0l36.203 36.204c9.997 9.997 9.997 26.206.0 36.204l-294.4 294.401c-9.998 9.997-26.207 9.997-36.204-.001z"/></svg></span>
</button>
<button class="tw-select-none
tw-mx-2
tw-block
group-[.is-open]:tw-hidden
print:!tw-hidden" disabled aria-hidden=true><svg class="icon" viewBox="0 0 512 512"><path d="M328 256c0 39.8-32.2 72-72 72s-72-32.2-72-72 32.2-72 72-72 72 32.2 72 72zm104-72c-39.8.0-72 32.2-72 72s32.2 72 72 72 72-32.2 72-72-32.2-72-72-72zm-352 0c-39.8.0-72 32.2-72 72s32.2 72 72 72 72-32.2 72-72-32.2-72-72-72z"/></svg></button></div></div><pre style=counter-reset:codeblock class="tw-block tw-m-0 tw-p-0"><code id=codeblock-id-3 class="chroma
!tw-block
tw-p-0
tw-m-0
tw-transition-[max-height]
tw-duration-500
tw-ease-in-out
group-[.is-closed]:!tw-max-h-0
group-[.is-wrap]:tw-text-wrap
tw-overflow-y-hidden
tw-overflow-x-auto
tw-scrollbar-thin"><span class=line><span class=cl>John bought a car. He loves it.
</span></span><span class=line><span class=cl> ↓                ↓  ↓
</span></span><span class=line><span class=cl> └────────────────┘──┘
</span></span><span class=line><span class=cl>       指代关系</span></span></code></pre></div><ol start=3><li><strong>长距离依赖</strong>：</li></ol><div class="code-block highlight is-open show-line-numbers tw-group tw-my-2"><div class="tw-flex
tw-flex-row
tw-flex-1
tw-justify-between
tw-w-full tw-bg-bgColor-secondary"><button class="code-block-button
tw-mx-2
tw-flex
tw-flex-row
tw-flex-1" aria-hidden=true><div class="group-[.is-open]:tw-rotate-90 tw-transition-[transform] tw-duration-500 tw-ease-in-out print:!tw-hidden tw-w-min tw-h-min tw-my-1 tw-mx-1"><svg class="icon" viewBox="0 0 320 512"><path d="M285.476 272.971 91.132 467.314c-9.373 9.373-24.569 9.373-33.941.0l-22.667-22.667c-9.357-9.357-9.375-24.522-.04-33.901L188.505 256 34.484 101.255c-9.335-9.379-9.317-24.544.04-33.901l22.667-22.667c9.373-9.373 24.569-9.373 33.941.0L285.475 239.03c9.373 9.372 9.373 24.568.001 33.941z"/></svg></div><p class="tw-select-none !tw-my-1">text</p></button><div class=tw-flex><button class="line-number-button
tw-mx-2
tw-hidden
group-[.is-open]:tw-block
group-[.show-line-numbers]:tw-text-fgColor-link
print:!tw-hidden" title="Toggle line numbers"><svg class="icon" viewBox="0 0 512 512"><path d="M61.77 401l17.5-20.15a19.92 19.92.0 005.07-14.19v-3.31C84.34 356 80.5 352 73 352H16a8 8 0 00-8 8v16a8 8 0 008 8h22.83a157.41 157.41.0 00-11 12.31l-5.61 7c-4 5.07-5.25 10.13-2.8 14.88l1.05 1.93c3 5.76 6.29 7.88 12.25 7.88h4.73c10.33.0 15.94 2.44 15.94 9.09.0 4.72-4.2 8.22-14.36 8.22a41.54 41.54.0 01-15.47-3.12c-6.49-3.88-11.74-3.5-15.6 3.12l-5.59 9.31c-3.72 6.13-3.19 11.72 2.63 15.94 7.71 4.69 20.38 9.44 37 9.44 34.16.0 48.5-22.75 48.5-44.12-.03-14.38-9.12-29.76-28.73-34.88zM496 224H176a16 16 0 00-16 16v32a16 16 0 0016 16h320a16 16 0 0016-16v-32a16 16 0 00-16-16zm0-160H176a16 16 0 00-16 16v32a16 16 0 0016 16h320a16 16 0 0016-16V80a16 16 0 00-16-16zm0 320H176a16 16 0 00-16 16v32a16 16 0 0016 16h320a16 16 0 0016-16v-32a16 16 0 00-16-16zM16 160h64a8 8 0 008-8v-16a8 8 0 00-8-8H64V40a8 8 0 00-8-8H32a8 8 0 00-7.14 4.42l-8 16A8 8 0 0024 64h8v64H16a8 8 0 00-8 8v16a8 8 0 008 8zm-3.91 160H80a8 8 0 008-8v-16a8 8 0 00-8-8H41.32c3.29-10.29 48.34-18.68 48.34-56.44.0-29.06-25-39.56-44.47-39.56-21.36.0-33.8 10-40.46 18.75-4.37 5.59-3 10.84 2.8 15.37l8.58 6.88c5.61 4.56 11 2.47 16.12-2.44a13.44 13.44.0 019.46-3.84c3.33.0 9.28 1.56 9.28 8.75C51 248.19.0 257.31.0 304.59v4C0 316 5.08 320 12.09 320z"/></svg></button>
<button class="wrap-code-button
tw-select-none
tw-mx-2
tw-hidden
group-[.is-open]:tw-block
group-[.is-wrap]:tw-text-fgColor-link
print:!tw-hidden" title="Toggle code wrap"><svg class="icon" viewBox="0 0 448 512"><path d="M16 132h416c8.837.0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163.0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837.0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837.0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837.0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837.0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"/></svg></button>
<button class="copy-code-button
tw-select-none
tw-mx-2
tw-hidden
group-[.is-open]:tw-block
hover:tw-text-fgColor-link
print:!tw-hidden" title="Copy code">
<span class="copy-icon tw-block"><svg class="icon" viewBox="0 0 448 512"><path d="M433.941 65.941l-51.882-51.882A48 48 0 00348.118.0H176c-26.51.0-48 21.49-48 48v48H48c-26.51.0-48 21.49-48 48v320c0 26.51 21.49 48 48 48h224c26.51.0 48-21.49 48-48v-48h80c26.51.0 48-21.49 48-48V99.882a48 48 0 00-14.059-33.941zM266 464H54a6 6 0 01-6-6V150a6 6 0 016-6h74v224c0 26.51 21.49 48 48 48h96v42a6 6 0 01-6 6zm128-96H182a6 6 0 01-6-6V54a6 6 0 016-6h106v88c0 13.255 10.745 24 24 24h88v202a6 6 0 01-6 6zm6-256h-64V48h9.632c1.591.0 3.117.632 4.243 1.757l48.368 48.368a6 6 0 011.757 4.243V112z"/></svg></span>
<span class="check-icon tw-hidden"><svg class="icon" viewBox="0 0 512 512"><path d="M173.898 439.404l-166.4-166.4c-9.997-9.997-9.997-26.206.0-36.204l36.203-36.204c9.997-9.998 26.207-9.998 36.204.0L192 312.69 432.095 72.596c9.997-9.997 26.207-9.997 36.204.0l36.203 36.204c9.997 9.997 9.997 26.206.0 36.204l-294.4 294.401c-9.998 9.997-26.207 9.997-36.204-.001z"/></svg></span>
</button>
<button class="tw-select-none
tw-mx-2
tw-block
group-[.is-open]:tw-hidden
print:!tw-hidden" disabled aria-hidden=true><svg class="icon" viewBox="0 0 512 512"><path d="M328 256c0 39.8-32.2 72-72 72s-72-32.2-72-72 32.2-72 72-72 72 32.2 72 72zm104-72c-39.8.0-72 32.2-72 72s32.2 72 72 72 72-32.2 72-72-32.2-72-72-72zm-352 0c-39.8.0-72 32.2-72 72s32.2 72 72 72 72-32.2 72-72-32.2-72-72-72z"/></svg></button></div></div><pre style=counter-reset:codeblock class="tw-block tw-m-0 tw-p-0"><code id=codeblock-id-4 class="chroma
!tw-block
tw-p-0
tw-m-0
tw-transition-[max-height]
tw-duration-500
tw-ease-in-out
group-[.is-closed]:!tw-max-h-0
group-[.is-wrap]:tw-text-wrap
tw-overflow-y-hidden
tw-overflow-x-auto
tw-scrollbar-thin"><span class=line><span class=cl>Although it was raining hard, ... we still went out.
</span></span><span class=line><span class=cl> ↓                                           ↓
</span></span><span class=line><span class=cl> └───────────────────────────────────────────┘
</span></span><span class=line><span class=cl>            让步关系</span></span></code></pre></div><h3 id=-注意力机制的优势 class=headerLink><a href=#-%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%e7%9a%84%e4%bc%98%e5%8a%bf class=header-mark></a>⚡ 注意力机制的优势</h3><ol><li><p><strong>计算效率</strong>：</p><ul><li>复杂度：O(n²)，但可以并行计算</li><li>相比 RNN 的 O(n)序列依赖，训练速度更快</li></ul></li><li><p><strong>建模能力</strong>：</p><ul><li>任意两个词之间直接连接</li><li>无距离衰减，完美捕捉长距离依赖</li></ul></li><li><p><strong>可解释性</strong>：</p><ul><li>注意力权重可视化</li><li>帮助理解模型决策过程</li></ul></li><li><p><strong>灵活性</strong>：</p><ul><li>可以处理不同长度的序列</li><li>易于与其他机制结合</li></ul></li></ol><h3 id=-注意力机制的局限性 class=headerLink><a href=#-%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%e7%9a%84%e5%b1%80%e9%99%90%e6%80%a7 class=header-mark></a>🎯 注意力机制的局限性</h3><ol><li><strong>计算复杂度</strong>：O(n²)对长序列不友好</li><li><strong>位置信息丢失</strong>：需要额外位置编码</li><li><strong>噪声敏感</strong>：可能关注不相关的词</li><li><strong>理论解释</strong>：与人类注意力的差异</li></ol><h3 id=-实际代码示例简化版 class=headerLink><a href=#-%e5%ae%9e%e9%99%85%e4%bb%a3%e7%a0%81%e7%a4%ba%e4%be%8b%e7%ae%80%e5%8c%96%e7%89%88 class=header-mark></a>🧪 实际代码示例（简化版）</h3><div class="code-block highlight is-closed show-line-numbers tw-group tw-my-2"><div class="tw-flex
tw-flex-row
tw-flex-1
tw-justify-between
tw-w-full tw-bg-bgColor-secondary"><button class="code-block-button
tw-mx-2
tw-flex
tw-flex-row
tw-flex-1" aria-hidden=true><div class="group-[.is-open]:tw-rotate-90 tw-transition-[transform] tw-duration-500 tw-ease-in-out print:!tw-hidden tw-w-min tw-h-min tw-my-1 tw-mx-1"><svg class="icon" viewBox="0 0 320 512"><path d="M285.476 272.971 91.132 467.314c-9.373 9.373-24.569 9.373-33.941.0l-22.667-22.667c-9.357-9.357-9.375-24.522-.04-33.901L188.505 256 34.484 101.255c-9.335-9.379-9.317-24.544.04-33.901l22.667-22.667c9.373-9.373 24.569-9.373 33.941.0L285.475 239.03c9.373 9.372 9.373 24.568.001 33.941z"/></svg></div><p class="tw-select-none !tw-my-1">python</p></button><div class=tw-flex><button class="line-number-button
tw-mx-2
tw-hidden
group-[.is-open]:tw-block
group-[.show-line-numbers]:tw-text-fgColor-link
print:!tw-hidden" title="Toggle line numbers"><svg class="icon" viewBox="0 0 512 512"><path d="M61.77 401l17.5-20.15a19.92 19.92.0 005.07-14.19v-3.31C84.34 356 80.5 352 73 352H16a8 8 0 00-8 8v16a8 8 0 008 8h22.83a157.41 157.41.0 00-11 12.31l-5.61 7c-4 5.07-5.25 10.13-2.8 14.88l1.05 1.93c3 5.76 6.29 7.88 12.25 7.88h4.73c10.33.0 15.94 2.44 15.94 9.09.0 4.72-4.2 8.22-14.36 8.22a41.54 41.54.0 01-15.47-3.12c-6.49-3.88-11.74-3.5-15.6 3.12l-5.59 9.31c-3.72 6.13-3.19 11.72 2.63 15.94 7.71 4.69 20.38 9.44 37 9.44 34.16.0 48.5-22.75 48.5-44.12-.03-14.38-9.12-29.76-28.73-34.88zM496 224H176a16 16 0 00-16 16v32a16 16 0 0016 16h320a16 16 0 0016-16v-32a16 16 0 00-16-16zm0-160H176a16 16 0 00-16 16v32a16 16 0 0016 16h320a16 16 0 0016-16V80a16 16 0 00-16-16zm0 320H176a16 16 0 00-16 16v32a16 16 0 0016 16h320a16 16 0 0016-16v-32a16 16 0 00-16-16zM16 160h64a8 8 0 008-8v-16a8 8 0 00-8-8H64V40a8 8 0 00-8-8H32a8 8 0 00-7.14 4.42l-8 16A8 8 0 0024 64h8v64H16a8 8 0 00-8 8v16a8 8 0 008 8zm-3.91 160H80a8 8 0 008-8v-16a8 8 0 00-8-8H41.32c3.29-10.29 48.34-18.68 48.34-56.44.0-29.06-25-39.56-44.47-39.56-21.36.0-33.8 10-40.46 18.75-4.37 5.59-3 10.84 2.8 15.37l8.58 6.88c5.61 4.56 11 2.47 16.12-2.44a13.44 13.44.0 019.46-3.84c3.33.0 9.28 1.56 9.28 8.75C51 248.19.0 257.31.0 304.59v4C0 316 5.08 320 12.09 320z"/></svg></button>
<button class="wrap-code-button
tw-select-none
tw-mx-2
tw-hidden
group-[.is-open]:tw-block
group-[.is-wrap]:tw-text-fgColor-link
print:!tw-hidden" title="Toggle code wrap"><svg class="icon" viewBox="0 0 448 512"><path d="M16 132h416c8.837.0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163.0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837.0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837.0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837.0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837.0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"/></svg></button>
<button class="copy-code-button
tw-select-none
tw-mx-2
tw-hidden
group-[.is-open]:tw-block
hover:tw-text-fgColor-link
print:!tw-hidden" title="Copy code">
<span class="copy-icon tw-block"><svg class="icon" viewBox="0 0 448 512"><path d="M433.941 65.941l-51.882-51.882A48 48 0 00348.118.0H176c-26.51.0-48 21.49-48 48v48H48c-26.51.0-48 21.49-48 48v320c0 26.51 21.49 48 48 48h224c26.51.0 48-21.49 48-48v-48h80c26.51.0 48-21.49 48-48V99.882a48 48 0 00-14.059-33.941zM266 464H54a6 6 0 01-6-6V150a6 6 0 016-6h74v224c0 26.51 21.49 48 48 48h96v42a6 6 0 01-6 6zm128-96H182a6 6 0 01-6-6V54a6 6 0 016-6h106v88c0 13.255 10.745 24 24 24h88v202a6 6 0 01-6 6zm6-256h-64V48h9.632c1.591.0 3.117.632 4.243 1.757l48.368 48.368a6 6 0 011.757 4.243V112z"/></svg></span>
<span class="check-icon tw-hidden"><svg class="icon" viewBox="0 0 512 512"><path d="M173.898 439.404l-166.4-166.4c-9.997-9.997-9.997-26.206.0-36.204l36.203-36.204c9.997-9.998 26.207-9.998 36.204.0L192 312.69 432.095 72.596c9.997-9.997 26.207-9.997 36.204.0l36.203 36.204c9.997 9.997 9.997 26.206.0 36.204l-294.4 294.401c-9.998 9.997-26.207 9.997-36.204-.001z"/></svg></span>
</button>
<button class="tw-select-none
tw-mx-2
tw-block
group-[.is-open]:tw-hidden
print:!tw-hidden" disabled aria-hidden=true><svg class="icon" viewBox="0 0 512 512"><path d="M328 256c0 39.8-32.2 72-72 72s-72-32.2-72-72 32.2-72 72-72 72 32.2 72 72zm104-72c-39.8.0-72 32.2-72 72s32.2 72 72 72 72-32.2 72-72-32.2-72-72-72zm-352 0c-39.8.0-72 32.2-72 72s32.2 72 72 72 72-32.2 72-72-32.2-72-72-72z"/></svg></button></div></div><pre style=counter-reset:codeblock class="tw-block tw-m-0 tw-p-0"><code id=codeblock-id-5 class="chroma
!tw-block
tw-p-0
tw-m-0
tw-transition-[max-height]
tw-duration-500
tw-ease-in-out
group-[.is-closed]:!tw-max-h-0
group-[.is-wrap]:tw-text-wrap
tw-overflow-y-hidden
tw-overflow-x-auto
tw-scrollbar-thin"><span class=line><span class=cl><span class=k>def</span> <span class=nf>attention</span><span class=p>(</span><span class=n>Q</span><span class=p>,</span> <span class=n>K</span><span class=p>,</span> <span class=n>V</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 计算注意力得分</span>
</span></span><span class=line><span class=cl>    <span class=n>scores</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>Q</span><span class=p>,</span> <span class=n>K</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>scores</span> <span class=o>=</span> <span class=n>scores</span> <span class=o>/</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>d_k</span><span class=p>)</span>  <span class=c1># 缩放</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Softmax归一化</span>
</span></span><span class=line><span class=cl>    <span class=n>attn_weights</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>scores</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 加权求和</span>
</span></span><span class=line><span class=cl>    <span class=n>output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>attn_weights</span><span class=p>,</span> <span class=n>V</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>output</span><span class=p>,</span> <span class=n>attn_weights</span></span></span></code></pre></div><h3 id=4-前馈神经网络feed-forward-network class=headerLink><a href=#4-%e5%89%8d%e9%a6%88%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9cfeed-forward-network class=header-mark></a>4️⃣ 前馈神经网络（Feed-Forward Network）</h3><p>对每个词的上下文表示进行非线性变换（进一步提炼语义特征）。</p><h3 id=5-层归一化layer-normalization--残差连接residual-connection class=headerLink><a href=#5-%e5%b1%82%e5%bd%92%e4%b8%80%e5%8c%96layer-normalization--%e6%ae%8b%e5%b7%ae%e8%bf%9e%e6%8e%a5residual-connection class=header-mark></a>5️⃣ 层归一化（Layer Normalization） & 残差连接（Residual Connection）</h3><p>这两个是"稳定器"和"加速器&rdquo;，防止深层网络训练不稳定或梯度消失。</p><h3 id=6-编码器encoder--解码器decoder class=headerLink><a href=#6-%e7%bc%96%e7%a0%81%e5%99%a8encoder--%e8%a7%a3%e7%a0%81%e5%99%a8decoder class=header-mark></a>6️⃣ 编码器（Encoder） & 解码器（Decoder）</h3><p>经典 Transformer 分为两部分：</p><div class=table-wrapper><table><thead><tr><th style=text-align:>模块</th><th style=text-align:>作用</th><th style=text-align:>代表模型</th></tr></thead><tbody><tr><td style=text-align:><strong>Encoder</strong></td><td style=text-align:>把输入理解成语义向量（理解）</td><td style=text-align:>BERT</td></tr><tr><td style=text-align:><strong>Decoder</strong></td><td style=text-align:>根据上下文生成输出（生成）</td><td style=text-align:>GPT</td></tr><tr><td style=text-align:><strong>Encoder-Decoder</strong></td><td style=text-align:>两者兼有（翻译任务）</td><td style=text-align:>T5, MT5, Bard</td></tr></tbody></table></div><hr><h2 id=-五transformer-的运行流程以-gpt-为例 class=headerLink><a href=#-%e4%ba%94transformer-%e7%9a%84%e8%bf%90%e8%a1%8c%e6%b5%81%e7%a8%8b%e4%bb%a5-gpt-%e4%b8%ba%e4%be%8b class=header-mark></a>🔄 五、Transformer 的运行流程（以 GPT 为例）</h2><p>1️⃣ <strong>用户输入文字（Prompt）</strong>
👉 &ldquo;写一首关于春天的诗&rdquo;</p><p>2️⃣ <strong>模型将文字 Token 化</strong>
👉 [&ldquo;写&rdquo;, &ldquo;一首&rdquo;, &ldquo;关于&rdquo;, &ldquo;春天&rdquo;, &ldquo;的&rdquo;, &ldquo;诗&rdquo;]</p><p>3️⃣ <strong>每个 token 转为向量 → 加位置编码</strong>
👉 数学矩阵形式输入 Transformer 层堆栈</p><p>4️⃣ <strong>每一层执行以下操作</strong>：</p><ul><li>自注意力：理解上下文依赖</li><li>前馈网络：提炼语义</li><li>层归一化 + 残差：稳定训练</li></ul><p>5️⃣ <strong>最后一层输出每个 token 的概率分布</strong>
👉 模型根据概率<strong>逐 token 预测下一个字</strong></p><p>6️⃣ <strong>输出流式生成（decoding）</strong>
👉 &ldquo;春天的花开在风里，…&rdquo; 🌸</p><hr><h2 id=-六为什么-transformer-如此强大 class=headerLink><a href=#-%e5%85%ad%e4%b8%ba%e4%bb%80%e4%b9%88-transformer-%e5%a6%82%e6%ad%a4%e5%bc%ba%e5%a4%a7 class=header-mark></a>📈 六、为什么 Transformer 如此强大？</h2><div class=table-wrapper><table><thead><tr><th style=text-align:>优势</th><th style=text-align:>说明</th></tr></thead><tbody><tr><td style=text-align:>🚀 <strong>并行处理</strong></td><td style=text-align:>不像 RNN 一次一个字，Transformer 一次处理整段文本</td></tr><tr><td style=text-align:>🧠 <strong>长程依赖建模强</strong></td><td style=text-align:>注意力机制能捕捉远距离关系（如主语与谓语）</td></tr><tr><td style=text-align:>🌍 <strong>多任务适配性强</strong></td><td style=text-align:>只要换数据或指令就能做翻译、问答、代码生成等</td></tr><tr><td style=text-align:>🧩 <strong>可扩展性强</strong></td><td style=text-align:>层数、宽度、参数量可线性扩展（GPT-2→GPT-4）</td></tr><tr><td style=text-align:>💡 <strong>可解释性高</strong></td><td style=text-align:>注意力权重能显示模型"关注"了哪些词</td></tr></tbody></table></div><hr><h2 id=-七专业名词解释表 class=headerLink><a href=#-%e4%b8%83%e4%b8%93%e4%b8%9a%e5%90%8d%e8%af%8d%e8%a7%a3%e9%87%8a%e8%a1%a8 class=header-mark></a>📘 七、专业名词解释表</h2><blockquote><p>📖 <strong>详细的专业名词解释表已单独整理</strong>：请参考 <a href=https://byronfinn.github.io/ai%E4%B8%93%E4%B8%9A%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A%E8%A1%A8/ rel>AI 专业名词解释表</a></p></blockquote><p>本文涉及的核心概念包括：</p><ul><li>🏗️ <strong>架构技术</strong>：Transformer、注意力机制、位置编码等</li><li>🔢 <strong>数学表示</strong>：向量、嵌入、Query/Key/Value 等</li><li>🔄 <strong>处理流程</strong>：编码解码、层归一化、残差连接等</li></ul><p>所有相关术语的详细解释、通俗说明和实际举例都在专门的解释表中，便于系统学习和查阅。</p><hr><h2 id=-八一句话总结 class=headerLink><a href=#-%e5%85%ab%e4%b8%80%e5%8f%a5%e8%af%9d%e6%80%bb%e7%bb%93 class=header-mark></a>✨ 八、一句话总结</h2><blockquote><p><strong>Transformer 就是现代语言智能的"神经骨架"</strong>：它用注意力机制理解上下文，用层堆叠提炼语义，让模型能像人一样阅读、记忆和生成语言。</p></blockquote><hr><h2 id=-延伸阅读 class=headerLink><a href=#-%e5%bb%b6%e4%bc%b8%e9%98%85%e8%af%bb class=header-mark></a>📚 延伸阅读</h2><h3 id=-ai-大模型系统教程系列 class=headerLink><a href=#-ai-%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%b3%bb%e7%bb%9f%e6%95%99%e7%a8%8b%e7%b3%bb%e5%88%97 class=header-mark></a>🔗 AI 大模型系统教程系列</h3><ol><li><strong><a href=https://byronfinn.github.io/ai%E6%95%99%E7%A8%8B1/ rel>AI 大模型完全指南</a></strong> - 从零基础到 Token 与向量的深度解析</li><li><strong>[本文] Transformer 架构深度解析</strong> - 注意力机制与 AI 大模型的核心技术</li><li><strong><a href=https://byronfinn.github.io/ai%E6%95%99%E7%A8%8B3/ rel>Prompt Engineering 完全指南</a></strong> - 从提示工程到上下文工程的实战教程</li><li><strong><a href=https://byronfinn.github.io/ai%E4%B8%93%E4%B8%9A%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A%E8%A1%A8/ rel>AI 专业名词解释表</a></strong> - 270+术语完全指南与 AI 技术体系词典</li></ol><h3 id=-深入学习建议 class=headerLink><a href=#-%e6%b7%b1%e5%85%a5%e5%ad%a6%e4%b9%a0%e5%bb%ba%e8%ae%ae class=header-mark></a>🎯 深入学习建议</h3><ul><li><strong>基础先行</strong>：如果对 Token、向量等概念不熟悉，建议先阅读 AI 大模型完全指南</li><li><strong>实践结合</strong>：学习完 Transformer 原理后，结合 Prompt Engineering 进行实际开发</li><li><strong>术语查阅</strong>：遇到专业术语时，可随时查阅 AI 专业名词解释表</li></ul><hr></div><h2>相关内容</h2><div class=related-container><div class=related-item-container><h2 class=related-title><a href=/ai%E6%95%99%E7%A8%8B1/>AI大模型完全指南：从零基础到Token与向量的深度解析</a></h2></div><div class=related-item-container><h2 class=related-title><a href=/ai%E6%95%99%E7%A8%8B4/>CPU/GPU 与大模型训练</a></h2></div><div class=related-item-container><h2 class=related-title><a href=/ai%E4%B8%93%E4%B8%9A%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A%E8%A1%A8/>AI专业名词解释表：270+术语完全指南与AI技术体系词典</a></h2></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>更新于 2025-11-06</span></div><div class=post-info-license></div></div><div class="post-info-line print:!tw-hidden"><div class=post-info-md></div><div class=post-info-share><button title="分享到 Twitter" data-sharer=twitter data-url=https://byronfinn.github.io/ai%E6%95%99%E7%A8%8B2/ data-title=Transformer架构深度解析：注意力机制与AI大模型的核心技术 data-hashtags="Transformer架构,注意力机制,自注意力,多头注意力,Query Key Value,位置编码,AI大模型,深度学习,神经网络"><svg class="icon" viewBox="0 0 512 512"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></button><button title="分享到 Evernote" data-sharer=evernote data-url=https://byronfinn.github.io/ai%E6%95%99%E7%A8%8B2/ data-title=Transformer架构深度解析：注意力机制与AI大模型的核心技术><svg class="icon" viewBox="0 0 384 512"><path d="M120.82 132.21c1.6 22.31-17.55 21.59-21.61 21.59-68.93.0-73.64-1-83.58 3.34-.56.22-.74.0-.37-.37L123.79 46.45c.38-.37.6-.22.38.37-4.35 9.99-3.35 15.09-3.35 85.39zm79 308c-14.68-37.08 13-76.93 52.52-76.62 17.49.0 22.6 23.21 7.95 31.42-6.19 3.3-24.95 1.74-25.14 19.2-.05 17.09 19.67 25 31.2 24.89A45.64 45.64.0 00312 393.45v-.08c0-11.63-7.79-47.22-47.54-55.34-7.72-1.54-65-6.35-68.35-50.52-3.74 16.93-17.4 63.49-43.11 69.09-8.74 1.94-69.68 7.64-112.92-36.77.0.0-18.57-15.23-28.23-57.95-3.38-15.75-9.28-39.7-11.14-62 0-18 11.14-30.45 25.07-32.2 81 0 90 2.32 101-7.8 9.82-9.24 7.8-15.5 7.8-102.78 1-8.3 7.79-30.81 53.41-24.14 6 .86 31.91 4.18 37.48 30.64l64.26 11.15c20.43 3.71 70.94 7 80.6 57.94 22.66 121.09 8.91 238.46 7.8 238.46C362.15 485.53 267.06 480 267.06 480c-18.95-.23-54.25-9.4-67.27-39.83zm80.94-204.84c-1 1.92-2.2 6 .85 7 14.09 4.93 39.75 6.84 45.88 5.53 3.11-.25 3.05-4.43 2.48-6.65-3.53-21.85-40.83-26.5-49.24-5.92z"/></svg></button></div></div></div><div class=post-info-more><section class=post-tags><svg class="icon" viewBox="0 0 640 512"><path d="M497.941 225.941 286.059 14.059A48 48 0 00252.118.0H48C21.49.0.0 21.49.0 48v204.118a48 48 0 0014.059 33.941l211.882 211.882c18.744 18.745 49.136 18.746 67.882.0l204.118-204.118c18.745-18.745 18.745-49.137.0-67.882zM112 160c-26.51.0-48-21.49-48-48s21.49-48 48-48 48 21.49 48 48-21.49 48-48 48zm513.941 133.823L421.823 497.941c-18.745 18.745-49.137 18.745-67.882.0l-.36-.36L527.64 323.522c16.999-16.999 26.36-39.6 26.36-63.64s-9.362-46.641-26.36-63.64L331.397.0h48.721a48 48 0 0133.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137.0 67.882z"/></svg>&nbsp;<a href=/tags/transformer%E6%9E%B6%E6%9E%84/>Transformer架构</a>,&nbsp;<a href=/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/>注意力机制</a>,&nbsp;<a href=/tags/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B/>自注意力</a>,&nbsp;<a href=/tags/%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B/>多头注意力</a>,&nbsp;<a href=/tags/query-key-value/>Query Key Value</a>,&nbsp;<a href=/tags/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/>位置编码</a>,&nbsp;<a href=/tags/ai%E5%A4%A7%E6%A8%A1%E5%9E%8B/>AI大模型</a>,&nbsp;<a href=/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a>,&nbsp;<a href=/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/>神经网络</a></section><section class=print:!tw-hidden><span><button class="tw-text-fgColor-link-muted hover:tw-text-fgColor-link-muted-hover" onclick=window.history.back()>返回</button></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class="post-nav print:tw-hidden"><a href=/ai%E6%95%99%E7%A8%8B3/ class=prev rel=prev title="Prompt Engineering完全指南：从提示工程到上下文工程的实战教程"><svg class="icon" viewBox="0 0 256 512"><path d="M31.7 239l136-136c9.4-9.4 24.6-9.4 33.9.0l22.6 22.6c9.4 9.4 9.4 24.6.0 33.9L127.9 256l96.4 96.4c9.4 9.4 9.4 24.6.0 33.9L201.7 409c-9.4 9.4-24.6 9.4-33.9.0l-136-136c-9.5-9.4-9.5-24.6-.1-34z"/></svg>Prompt Engineering完全指南：从提示工程到上下文工程的实战教程</a>
<a href=/ai%E6%95%99%E7%A8%8B1/ class=next rel=next title=AI大模型完全指南：从零基础到Token与向量的深度解析>AI大模型完全指南：从零基础到Token与向量的深度解析<svg class="icon" viewBox="0 0 256 512"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9.0l-22.6-22.6c-9.4-9.4-9.4-24.6.0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6.0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9.0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a></div></div><div id=comments class="print:!tw-hidden tw-pt-32 tw-pb-8"><div id=giscus></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://giscus.app/>giscus</a>.</noscript></div></article></main><footer class=footer><div class=footer-container><div class=footer-line><svg class="icon" viewBox="0 0 512 512"><path d="M256 8C119.033 8 8 119.033 8 256s111.033 248 248 248 248-111.033 248-248S392.967 8 256 8zm0 448c-110.532.0-2e2-89.451-2e2-2e2.0-110.531 89.451-2e2 2e2-2e2 110.532.0 2e2 89.451 2e2 2e2.0 110.532-89.451 2e2-2e2 2e2zm107.351-101.064c-9.614 9.712-45.53 41.396-104.065 41.396-82.43.0-140.484-61.425-140.484-141.567.0-79.152 60.275-139.401 139.762-139.401 55.531.0 88.738 26.62 97.593 34.779a11.965 11.965.0 011.936 15.322l-18.155 28.113c-3.841 5.95-11.966 7.282-17.499 2.921-8.595-6.776-31.814-22.538-61.708-22.538-48.303.0-77.916 35.33-77.916 80.082.0 41.589 26.888 83.692 78.277 83.692 32.657.0 56.843-19.039 65.726-27.225 5.27-4.857 13.596-4.039 17.82 1.738l19.865 27.17a11.947 11.947.0 01-1.152 15.518z"/></svg>2025<span class=author>&nbsp;<a href=https://blog.baifan.site target=_blank rel="noopener noreferrer">Finn</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class=footer-line></div><div class=footer-line></div></div><script>"serviceWorker"in navigator&&(navigator.serviceWorker.register("/sw.min.js",{scope:"/"}).then(function(){}),navigator.serviceWorker.ready.then(function(){}))</script></footer><div class="print:!tw-hidden tw-flex tw-flex-col tw-fixed tw-right-4 tw-bottom-4 tw-gap-2"><a href=#back-to-top id=back-to-top-button class="tw-transition-opacity tw-opacity-0 tw-block tw-bg-bgColor-secondary tw-rounded-full" style=padding:.6rem;line-height:1.3rem;font-size:1rem title=回到顶部><svg class="icon" viewBox="0 0 448 512"><path d="M34.9 289.5l-22.2-22.2c-9.4-9.4-9.4-24.6.0-33.9L207 39c9.4-9.4 24.6-9.4 33.9.0l194.3 194.3c9.4 9.4 9.4 24.6.0 33.9L413 289.4c-9.5 9.5-25 9.3-34.3-.4L264 168.6V456c0 13.3-10.7 24-24 24h-32c-13.3.0-24-10.7-24-24V168.6L69.2 289.1c-9.3 9.8-24.8 10-34.3.4z"/></svg>
</a><button id=toc-drawer-button class="tw-block tw-bg-bgColor-secondary tw-rounded-full md:tw-hidden" style=padding:.6rem;line-height:1.3rem;font-size:1rem>
<svg class="icon" viewBox="0 0 448 512"><path d="M16 132h416c8.837.0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163.0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837.0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837.0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837.0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837.0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"/></svg>
</button><a href=#comments id=view-comments class="tw-block tw-bg-bgColor-secondary tw-rounded-full" style=padding:.6rem;line-height:1.3rem;font-size:1rem title=查看评论>
<svg class="icon" viewBox="0 0 512 512"><path d="M256 32C114.6 32 0 125.1.0 240c0 49.6 21.4 95 57 130.7C44.5 421.1 2.7 466 2.2 466.5c-2.2 2.3-2.8 5.7-1.5 8.7S4.8 480 8 480c66.3.0 116-31.8 140.6-51.4 32.7 12.3 69 19.4 107.4 19.4 141.4.0 256-93.1 256-208S397.4 32 256 32z"/></svg></a></div><div id=cookieconsent-container></div><link rel=stylesheet href=/lib/katex/katex.min.0c8126645bb983a788b167b1b97abe2505a962ad45e049001463c46012012a9b.css integrity="sha256-DIEmZFu5g6eIsWexuXq+JQWpYq1F4EkAFGPEYBIBKps="><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/katex/copy-tex.min.cf8e3e934d92a839d209ffdac331fa693da5958a6dff2c8788a4713cc1f50a47.css integrity="sha256-z44+k02SqDnSCf/awzH6aT2llYpt/yyHiKRxPMH1Ckc="><noscript><link rel=stylesheet href=/lib/katex/copy-tex.min.cf8e3e934d92a839d209ffdac331fa693da5958a6dff2c8788a4713cc1f50a47.css integrity="sha256-z44+k02SqDnSCf/awzH6aT2llYpt/yyHiKRxPMH1Ckc="></noscript><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.cd0d0b6e50ff01ff2f3a9a70d7cfb66a7c6cb9acf7a566325568be6d3bd31fc4.css integrity="sha256-zQ0LblD/Af8vOppw18+2anxsuaz3pWYyVWi+bTvTH8Q="><script>window.config={"autocomplete.min.js":"/lib/autocomplete/autocomplete.min.js",comment:{giscus:{darkTheme:"dark",dataCategory:"Announcements",dataCategoryId:"DIC_kwDOQOVlP84Cxa17",dataEmitMetadata:"0",dataInputPosition:"top",dataLang:"zh-CN",dataLoading:"lazy",dataMapping:"pathname",dataReactionsEnabled:"1",dataRepo:"ByronFinn/ByronFinn.github.io",dataRepoId:"R_kgDOQOVlPw",dataStrict:"0",lightTheme:"light"}},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验."},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},data:{"desktop-header-typeit":"Daily Deep Think","mobile-header-typeit":"Daily Deep Think"},"fuse.min.js":"/lib/fuse/fuse.min.js",math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{distance:100,findAllMatches:!1,fuseIndexURL:"/index.json",highlightTag:"em",ignoreFieldNorm:!1,ignoreLocation:!1,isCaseSensitive:!1,location:0,maxResultLength:10,minMatchCharLength:2,noResultsFound:"没有找到结果",snippetLength:50,threshold:.3,type:"fuse",useExtendedSearch:!1},sharerjs:!0,table:{sort:!0},twemoji:!0,typeit:{cursorChar:"|",cursorSpeed:1e3,data:{"desktop-header-typeit":["desktop-header-typeit"],"mobile-header-typeit":["mobile-header-typeit"]},duration:-1,speed:100}}</script><script src=/lib/tablesort/tablesort.min.92de6dec051677787aed63503575b2f9be73f21f2745574e59647bc139a92d40.js integrity="sha256-kt5t7AUWd3h67WNQNXWy+b5z8h8nRVdOWWR7wTmpLUA="></script><script src=/lib/twemoji/twemoji.min.0e0e5259e3ff8ea805e0c5660c6336f7f46b14332e3cafb82939e1db3da8b6f8.js integrity="sha256-Dg5SWeP/jqgF4MVmDGM29/RrFDMuPK+4KTnh2z2otvg=" defer></script><script src=/js/twemoji.min.js defer></script><script src=/lib/sharer/sharer.min.8fe10eb615eb163a20f795484430a012805ec7c8c11df52df54ddb7a46084254.js integrity="sha256-j+EOthXrFjog95VIRDCgEoBex8jBHfUt9U3bekYIQlQ="></script><script src=/lib/typeit/typeit.min.06e0b9ba7bb3c9368aa26979037019306fef8e43dd2b9276854d227381445d0f.js integrity="sha256-BuC5unuzyTaKoml5A3AZMG/vjkPdK5J2hU0ic4FEXQ8="></script><script src=/lib/katex/katex.min.76d534cf1167067008fca12c4e903fc44cf8cfda8c5279c318d1f78cd90b086e.js integrity="sha256-dtU0zxFnBnAI/KEsTpA/xEz4z9qMUnnDGNH3jNkLCG4=" defer></script><script src=/lib/katex/auto-render.min.bb53eb953394531aae36fdd537065c4244eb8542901a3ce914601d932675b8ac.js integrity="sha256-u1PrlTOUUxquNv3VNwZcQkTrhUKQGjzpFGAdkyZ1uKw=" defer></script><script src=/lib/katex/copy-tex.min.07770af90943a1de1a1010794bc78c6a7346d46d48fb63e35cc76ba76b827604.js integrity="sha256-B3cK+QlDod4aEBB5S8eManNG1G1I+2PjXMdrp2uCdgQ=" defer></script><script src=/lib/katex/mhchem.min.9f87e5e9c384a160472d0045035a8641f6013358eddb3ece708634a50f946a40.js integrity="sha256-n4fl6cOEoWBHLQBFA1qGQfYBM1jt2z7OcIY0pQ+UakA=" defer></script><script src=/js/katex.min.js defer></script><script src=/lib/cookieconsent/cookieconsent.min.e55842a856a6d829feca3c3ad736c136b6c7549e9247274f78aa296259e06e24.js integrity="sha256-5VhCqFam2Cn+yjw61zbBNrbHVJ6SRydPeKopYlngbiQ=" defer></script><script src=/js/cookieconsent.min.js defer></script><script src=/js/theme.min.js defer></script><script src=/js/giscus.min.js defer></script><script type=speculationrules>
  {
    "prerender": [
      {
        "where": { "href_matches": "/*" },
        "eagerness": "moderate"
      }
    ]
  }
</script></body></html>