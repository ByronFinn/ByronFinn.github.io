<!doctype html><html lang=zh-CN><head><meta name=generator content="Hugo 0.152.2"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>第 3 页 - 每日深度思考 | 技术、经济分析与深度思考</title><meta name=Description content="专注于技术、经济、时事解读的深度思考博客，提供专业的技术洞察和商业分析"><meta property="og:url" content="https://byronfinn.github.io/"><meta property="og:site_name" content="每日深度思考 | 技术、经济分析与深度思考"><meta property="og:title" content="每日深度思考 | 技术、经济分析与深度思考"><meta property="og:description" content="专注于技术、经济、时事解读的深度思考博客，提供专业的技术洞察和商业分析"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="website"><meta property="og:image" content="https://byronfinn.github.io/pictures/avatar/angryCat.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://byronfinn.github.io/pictures/avatar/angryCat.png"><meta name=twitter:title content="每日深度思考 | 技术、经济分析与深度思考"><meta name=twitter:description content="专注于技术、经济、时事解读的深度思考博客，提供专业的技术洞察和商业分析"><meta name=application-name content="Daily Deep Think"><meta name=apple-mobile-web-app-title content="Daily Deep Think"><meta name=theme-color content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel=icon href=/static/icos/favicon.svg><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://byronfinn.github.io/page/3/><link rel=alternate href=/index.xml type=application/rss+xml title="每日深度思考 | 技术、经济分析与深度思考"><link rel=feed href=/index.xml type=application/rss+xml title="每日深度思考 | 技术、经济分析与深度思考"><link rel=stylesheet href=/css/main.min.css><link rel=stylesheet href=/css/style.min.css><meta name=google-site-verification content="google8f3e688b6959e353"><meta name=msvalidate.01 content="请在此添加你的Bing验证码"><meta name=yandex-verification content="请在此添加你的Yandex验证码"><meta name=p:domain_verify content="请在此添加你的Pinterest验证码"><meta name=baidu-site-verification content="请在此添加你的百度验证码"><meta name=sogou_site_verification content="请在此添加你的搜狗验证码"><meta name=360-site-verification content="请在此添加你的360搜索验证码"><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"BlogPosting","headline":"CPU/GPU 与大模型训练","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https://byronfinn.github.io/ai%E6%95%99%E7%A8%8B4/"},"image":["https://byronfinn.github.io/pictures/avatar/angryCat.png"],"genre":"posts","keywords":["GPU训练","CUDA编程","张量精度","模型量化","深度学习硬件","PyTorch教程","AI模型训练","显存优化","硬件选型","AI实战"],"wordcount":3592,"url":"https://byronfinn.github.io/ai%E6%95%99%E7%A8%8B4/","datePublished":"2025-11-06T09:36:42+08:00","dateModified":"2025-11-06T09:36:42+08:00","publisher":{"@type":"Organization","name":"Finn"},"author":{"@type":"Person","name":"Finn","url":"https://blog.baifan.site"},"description":"AI教程第四篇：深度学习GPU加速实战指南。涵盖CPU/GPU架构对比、张量与精度量化、CUDA编程实战、PyTorch训练工作流、硬件选型与显存优化，包含面试问答与排错清单，助你掌握AI模型训练的核心工程技能。"},{"@type":"BlogPosting","headline":"RAG系统完全指南——从零搭建本地检索增强生成系统","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https://byronfinn.github.io/ai%E6%95%99%E7%A8%8B5/"},"image":["https://byronfinn.github.io/pictures/avatar/angryCat.png"],"genre":"posts","keywords":["RAG","检索增强生成","LangChain","Ollama","pgvector","向量数据库","本地LLM","文档问答","AI实战","RAG系统"],"wordcount":5580,"url":"https://byronfinn.github.io/ai%E6%95%99%E7%A8%8B5/","datePublished":"2025-11-06T09:36:42+08:00","dateModified":"2025-11-06T09:36:42+08:00","publisher":{"@type":"Organization","name":"Finn"},"author":{"@type":"Person","name":"Finn","url":"https://blog.baifan.site"},"description":"AI教程第五篇：RAG系统完全指南。深入讲解LangChain+Ollama+pgvector搭建本地RAG系统，涵盖文档切分、向量化、检索优化、提示工程等核心技术。包含完整实战代码、面试指南和排错清单，助你掌握企业级RAG应用开发。"},{"@type":"BlogPosting","headline":"当TikTok遇上特朗普：做多错多，何以关关难过关关过","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https://byronfinn.github.io/tiktok%E7%9A%84%E5%9B%B0%E5%B1%80/"},"image":["https://byronfinn.github.io/pictures/avatar/angryCat.png"],"genre":"posts","keywords":["TikTok","特朗普","地缘政治","字节跳动","中美关系","科技政治","社交媒体","企业管理","权斗分析","Emily Baker-White","Every Screen on the Planet"],"wordcount":3896,"url":"https://byronfinn.github.io/tiktok%E7%9A%84%E5%9B%B0%E5%B1%80/","datePublished":"2025-11-05T19:45:00+08:00","dateModified":"2025-11-05T19:45:00+08:00","publisher":{"@type":"Organization","name":"Finn"},"author":{"@type":"Person","name":"Finn","url":"https://blog.baifan.site"},"description":"深度解析TikTok在特朗普政府时期的生存策略、内部管理危机和地缘政治博弈。基于Emily Baker-White新书《Every Screen on the Planet》的调查报道，揭示TikTok内部权斗和管理层误判如何影响其在美发展，探讨科技公司在大国博弈中的生存之道。"},{"@type":"BlogPosting","headline":"AI专业名词解释表：270+术语完全指南与AI技术体系词典","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https://byronfinn.github.io/ai%E4%B8%93%E4%B8%9A%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A%E8%A1%A8/"},"image":["https://byronfinn.github.io/pictures/avatar/angryCat.png"],"genre":"posts","keywords":["AI专业名词","人工智能","机器学习","深度学习","Transformer","LLM大模型","Prompt工程","RAG技术","向量数据库","注意力机制"],"wordcount":6756,"url":"https://byronfinn.github.io/ai%E4%B8%93%E4%B8%9A%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A%E8%A1%A8/","datePublished":"2025-11-05T09:58:24+08:00","dateModified":"2025-11-05T09:58:24+08:00","publisher":{"@type":"Organization","name":"Finn"},"author":{"@type":"Person","name":"Finn","url":"https://blog.baifan.site"},"description":"最全面的AI专业名词解释表，涵盖270+个AI术语：从Token、Transformer到RAG、Prompt工程。系统学习AI大模型技术体系，包含12大分类和A-Z速查表，是AI学习者的必备词典。"},{"@type":"BlogPosting","headline":"Prompt Engineering完全指南：从提示工程到上下文工程的实战教程","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https://byronfinn.github.io/ai%E6%95%99%E7%A8%8B3/"},"image":["https://byronfinn.github.io/pictures/avatar/angryCat.png"],"genre":"posts","keywords":["Prompt Engineering","Context Engineering","提示词工程","上下文工程","RAG","检索增强生成","AI应用开发","LLM优化","上下文管理","人工智能"],"wordcount":3306,"url":"https://byronfinn.github.io/ai%E6%95%99%E7%A8%8B3/","datePublished":"2025-11-05T09:58:24+08:00","dateModified":"2025-11-05T09:58:24+08:00","publisher":{"@type":"Organization","name":"Finn"},"author":{"@type":"Person","name":"Finn","url":"https://blog.baifan.site"},"description":"全面掌握Prompt Engineering与Context Engineering核心技术：从基础提示词设计到高级上下文管理，包括RAG、上下文优化、持久化等技术。解决实际开发中的污染问题、注意力偏移等挑战，提升AI应用效果。"},{"@type":"BlogPosting","headline":"Transformer架构深度解析：注意力机制与AI大模型的核心技术","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https://byronfinn.github.io/ai%E6%95%99%E7%A8%8B2/"},"image":["https://byronfinn.github.io/pictures/avatar/angryCat.png"],"genre":"posts","keywords":["Transformer","注意力机制","自注意力","多头注意力","Query Key Value","位置编码","AI架构","深度学习","神经网络","GPT","BERT"],"wordcount":3748,"url":"https://byronfinn.github.io/ai%E6%95%99%E7%A8%8B2/","datePublished":"2025-11-05T09:58:24+08:00","dateModified":"2025-11-05T09:58:24+08:00","publisher":{"@type":"Organization","name":"Finn"},"author":{"@type":"Person","name":"Finn","url":"https://blog.baifan.site"},"description":"深入解析Transformer架构核心技术：自注意力机制、多头注意力、位置编码等核心组件。详细阐述Query/Key/Value原理，理解GPT、BERT等大模型的技术基础，掌握现代AI的核心架构。"}]}</script></head><body data-instant-intensity=viewport class="tw-flex tw-min-h-screen tw-flex-col"><script>function setTheme(e){document.body.setAttribute("theme",e),document.documentElement.className=e,document.documentElement.style.setProperty("color-scheme",e==="light"?"light":"dark"),e==="light"?document.documentElement.classList.remove("tw-dark"):document.documentElement.classList.add("tw-dark"),window.theme=e,window.isDark=window.theme!=="light"}function saveTheme(e){window.localStorage&&localStorage.setItem("theme",e)}function getMeta(e){const t=document.getElementsByTagName("meta");for(let n=0;n<t.length;n++)if(t[n].getAttribute("name")===e)return t[n];return""}if(window.localStorage&&localStorage.getItem("theme")){let e=localStorage.getItem("theme");e==="light"||e==="dark"?setTheme(e):setTheme(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")}else"auto"==="light"||"auto"==="dark"?(setTheme("auto"),saveTheme("auto")):(saveTheme("auto"),setTheme(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"));let metaColors={light:"#f8f8f8",dark:"#161b22"};getMeta("theme-color").content=metaColors[document.body.getAttribute("theme")],window.switchThemeEventSet=new Set</script><div id=back-to-top></div><div id=mask></div><header class="desktop print:!tw-hidden" id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="每日深度思考 | 技术、经济分析与深度思考"><span id=desktop-header-typeit class=typeit></span></a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>文章 </a><a class=menu-item href=/tags/>标签 </a><a class=menu-item href=/categories/>分类 </a><a class=menu-item href=/profile/ title=了解我的个人信息和专业背景>关于我 </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder=搜索... id=search-input-desktop>
<button class="search-button search-toggle" id=search-toggle-desktop title=搜索>
<svg class="icon" viewBox="0 0 512 512"><path d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</button>
<button class="search-button search-clear" id=search-clear-desktop title=清空>
<svg class="icon" viewBox="0 0 512 512"><path d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm121.6 313.1c4.7 4.7 4.7 12.3.0 17L338 377.6c-4.7 4.7-12.3 4.7-17 0L256 312l-65.1 65.6c-4.7 4.7-12.3 4.7-17 0L134.4 338c-4.7-4.7-4.7-12.3.0-17l65.6-65-65.6-65.1c-4.7-4.7-4.7-12.3.0-17l39.6-39.6c4.7-4.7 12.3-4.7 17 0l65 65.7 65.1-65.6c4.7-4.7 12.3-4.7 17 0l39.6 39.6c4.7 4.7 4.7 12.3.0 17L312 256l65.6 65.1z"/></svg>
</button>
<span class="search-button search-loading tw-animate-spin" id=search-loading-desktop><svg class="icon" viewBox="0 0 512 512"><path d="M304 48c0 26.51-21.49 48-48 48s-48-21.49-48-48 21.49-48 48-48 48 21.49 48 48zm-48 368c-26.51.0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zm208-208c-26.51.0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zM96 256c0-26.51-21.49-48-48-48S0 229.49.0 256s21.49 48 48 48 48-21.49 48-48zm12.922 99.078c-26.51.0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48c0-26.509-21.491-48-48-48zm294.156.0c-26.51.0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48c0-26.509-21.49-48-48-48zM108.922 60.922c-26.51.0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.491-48-48-48z"/></svg>
</span></span><button class="menu-item theme-switch" aria-label=切换主题>
<svg class="icon" viewBox="0 0 512 512"><path d="M8 256c0 136.966 111.033 248 248 248s248-111.034 248-248S392.966 8 256 8 8 119.033 8 256zm248 184V72c101.705.0 184 82.311 184 184 0 101.705-82.311 184-184 184z"/></svg></button></div></div></div></header><header class="mobile print:!tw-hidden" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="每日深度思考 | 技术、经济分析与深度思考"><span id=mobile-header-typeit class=typeit></span></a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索... id=search-input-mobile>
<button class="search-button search-toggle tw-h-10" id=search-toggle-mobile title=搜索>
<svg class="icon" viewBox="0 0 512 512"><path d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</button>
<button class="search-button search-clear tw-h-fit" id=search-clear-mobile title=清空>
<svg class="icon" viewBox="0 0 512 512"><path d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm121.6 313.1c4.7 4.7 4.7 12.3.0 17L338 377.6c-4.7 4.7-12.3 4.7-17 0L256 312l-65.1 65.6c-4.7 4.7-12.3 4.7-17 0L134.4 338c-4.7-4.7-4.7-12.3.0-17l65.6-65-65.6-65.1c-4.7-4.7-4.7-12.3.0-17l39.6-39.6c4.7-4.7 12.3-4.7 17 0l65 65.7 65.1-65.6c4.7-4.7 12.3-4.7 17 0l39.6 39.6c4.7 4.7 4.7 12.3.0 17L312 256l65.6 65.1z"/></svg>
</button>
<span class="search-button search-loading tw-animate-spin" id=search-loading-mobile><svg class="icon" viewBox="0 0 512 512"><path d="M304 48c0 26.51-21.49 48-48 48s-48-21.49-48-48 21.49-48 48-48 48 21.49 48 48zm-48 368c-26.51.0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zm208-208c-26.51.0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zM96 256c0-26.51-21.49-48-48-48S0 229.49.0 256s21.49 48 48 48 48-21.49 48-48zm12.922 99.078c-26.51.0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48c0-26.509-21.491-48-48-48zm294.156.0c-26.51.0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48c0-26.509-21.49-48-48-48zM108.922 60.922c-26.51.0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.491-48-48-48z"/></svg></span></div><button class=search-cancel id=search-cancel-mobile>
取消</button></div><a class=menu-item href=/posts/ title>文章</a><a class=menu-item href=/tags/ title>标签</a><a class=menu-item href=/categories/ title>分类</a><a class=menu-item href=/profile/ title=了解我的个人信息和专业背景>关于我</a><button class="menu-item theme-switch tw-w-full" aria-label=切换主题>
<svg class="icon" viewBox="0 0 512 512"><path d="M8 256c0 136.966 111.033 248 248 248s248-111.034 248-248S392.966 8 256 8 8 119.033 8 256zm248 184V72c101.705.0 184 82.311 184 184 0 101.705-82.311 184-184 184z"/></svg></button></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class="tw-mx-4 tw-flex-1"><div class="page home" posts><div class=home-profile><div class=home-avatar><a href=https://gravatar.com/peaceenthusiasticallye611d079ae title=文章 rel="noopener noreferrer" target=_blank><img loading=eager src=/8e06a54077f0073dac43a6bd8ddd3f01_6564130840018827055.png srcset="/8e06a54077f0073dac43a6bd8ddd3f01_6564130840018827055_hu_db526705dba0171f.webp 96w, /8e06a54077f0073dac43a6bd8ddd3f01_6564130840018827055_hu_9eab6a6e012dcfed.webp 144w, /8e06a54077f0073dac43a6bd8ddd3f01_6564130840018827055_hu_b4461a5746f8de8c.webp 192w" alt="Home avatar" height=100 width=100></a></div><h1 class=home-title>Daily Deep Think</h1><h2 class=home-subtitle><div id=id-3 class=typeit></div></h2><div class=links><a href=mailto:baifan@z.org title=Email rel=me><svg class="icon" viewBox="0 0 512 512"><path d="M464 64H48C21.49 64 0 85.49.0 112v288c0 26.51 21.49 48 48 48h416c26.51.0 48-21.49 48-48V112c0-26.51-21.49-48-48-48zm0 48v40.805c-22.422 18.259-58.168 46.651-134.587 106.49-16.841 13.247-50.201 45.072-73.413 44.701-23.208.375-56.579-31.459-73.413-44.701C106.18 199.465 70.425 171.067 48 152.805V112h416zM48 4e2V214.398c22.914 18.251 55.409 43.862 104.938 82.646 21.857 17.205 60.134 55.186 103.062 54.955 42.717.231 80.509-37.199 103.053-54.947 49.528-38.783 82.032-64.401 104.947-82.653V4e2H48z"/></svg></a></div><h3 class=home-disclaimer>发布的内容仅代表作者个人观点，不代表任何组织、公司或机构的立场</h3></div><article class="single summary"><h1 class=single-title><a href=/ai%E6%95%99%E7%A8%8B4/>CPU/GPU 与大模型训练</a></h1><div class=post-meta><span class=post-author><img class="tw-inline-block tw-max-h-4 tw-rounded-full tw-translate-y-[-2px] tw-mr-1" src=/pictures/avatar/angryCat.png alt="Finn avatar" height=16 width=16><a href=https://blog.baifan.site title=Author target=_blank rel="noopener noreferrer author" class=author>Finn</a>
</span>&nbsp;<span class=post-publish>发布于 <time datetime=2025-11-06>2025-11-06</time></span>&nbsp;<span class=post-category>收录于 </span>&nbsp;<span class=post-category>类别 <a href=/categories/ai%E6%8A%80%E6%9C%AF/><svg class="icon" viewBox="0 0 512 512"><path d="M464 128H272l-54.63-54.63c-6-6-14.14-9.37-22.63-9.37H48C21.49 64 0 85.49.0 112v288c0 26.51 21.49 48 48 48h416c26.51.0 48-21.49 48-48V176c0-26.51-21.49-48-48-48zm0 272H48V112h140.12l54.63 54.63c6 6 14.14 9.37 22.63 9.37H464v224z"/></svg>AI技术</a></span>&nbsp;<span class=post-category>和</span>&nbsp;<span class=post-series>系列 <a href><svg class="icon" viewBox="0 0 512 512"><path d="M464 32H48C21.49 32 0 53.49.0 80v352c0 26.51 21.49 48 48 48h416c26.51.0 48-21.49 48-48V80c0-26.51-21.49-48-48-48zm-6 4e2H54a6 6 0 01-6-6V86a6 6 0 016-6h404a6 6 0 016 6v340a6 6 0 01-6 6zm-42-92v24c0 6.627-5.373 12-12 12H204c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h2e2c6.627.0 12 5.373 12 12zm0-96v24c0 6.627-5.373 12-12 12H204c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h2e2c6.627.0 12 5.373 12 12zm0-96v24c0 6.627-5.373 12-12 12H204c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h2e2c6.627.0 12 5.373 12 12zm-252 12c0 19.882-16.118 36-36 36s-36-16.118-36-36 16.118-36 36-36 36 16.118 36 36zm0 96c0 19.882-16.118 36-36 36s-36-16.118-36-36 16.118-36 36-36 36 16.118 36 36zm0 96c0 19.882-16.118 36-36 36s-36-16.118-36-36 16.118-36 36-36 36 16.118 36 36z"/></svg></a></span></div><div class=content><h1 id=ai-教程-cpugpu-与大模型训练 class=headerLink><a href=#ai-%e6%95%99%e7%a8%8b-cpugpu-%e4%b8%8e%e5%a4%a7%e6%a8%a1%e5%9e%8b%e8%ae%ad%e7%bb%83 class=header-mark></a>AI 教程: CPU/GPU 与大模型训练</h1><blockquote><p>这是一份高浓缩资料：结构清晰、要点到位，涵盖 CPU/GPU 基础、张量与数值精度、CUDA 与 PyTorch 实操、硬件选型、常见问答与排错清单。</p></blockquote><hr><h2 id=0-速览30-秒 class=headerLink><a href=#0-%e9%80%9f%e8%a7%8830-%e7%a7%92 class=header-mark></a>0. 速览（30 秒）</h2><ul><li><strong>CPU vs GPU</strong>：CPU 擅长<strong>通用/顺序</strong>处理；GPU 擅长<strong>大规模并行</strong>（矩阵/向量）。</li><li><strong>大模型必备 GPU</strong>：训练/推理核心是矩阵乘和并行化，GPU 的高并发 + 高带宽显存恰好匹配。</li><li><strong>张量与精度</strong>：一切数据 → 张量；精度（FP16/FP8）与<strong>量化</strong>（INT8/INT4）是速度/显存与效果之间的权衡。</li><li><strong>PyTorch 上卡口诀</strong>：<code>device = "cuda" if ...; model.to(device); data.to(device)</code></li><li><strong>选卡看显存</strong>：先显存，再带宽/算力；生产尽量用<strong>满血高质量模型</strong>或云端托管 API。</li></ul><hr><h2 id=1-cpu-与-gpu差异场景与类比 class=headerLink><a href=#1-cpu-%e4%b8%8e-gpu%e5%b7%ae%e5%bc%82%e5%9c%ba%e6%99%af%e4%b8%8e%e7%b1%bb%e6%af%94 class=header-mark></a>1. CPU 与 GPU：差异、场景与类比</h2><h3 id=11-一句话对比 class=headerLink><a href=#11-%e4%b8%80%e5%8f%a5%e8%af%9d%e5%af%b9%e6%af%94 class=header-mark></a>1.1 一句话对比</h3><div class=table-wrapper><table><thead><tr><th style=text-align:>维度</th><th style=text-align:>CPU</th><th style=text-align:>GPU</th></tr></thead><tbody><tr><td style=text-align:>架构</td><td style=text-align:>少核、复杂控制流</td><td style=text-align:>海量小核、SIMT 并行</td></tr><tr><td style=text-align:>擅长</td><td style=text-align:>分支/系统任务/小规模计算</td><td style=text-align:>矩阵乘、卷积、注意力、图形渲染</td></tr><tr><td style=text-align:>任务模型</td><td style=text-align:>时间片轮转、低延迟切换</td><td style=text-align:>批处理&吞吐导向</td></tr><tr><td style=text-align:>典型用法</td><td style=text-align:>业务逻辑、调度、I/O</td><td style=text-align:>训练/推理主算子（GEMM、Conv 等）</td></tr></tbody></table></div><h3 id=12-形象类比 class=headerLink><a href=#12-%e5%bd%a2%e8%b1%a1%e7%b1%bb%e6%af%94 class=header-mark></a>1.2 形象类比</h3><ul><li><strong>CPU = 老专家</strong>：思考缜密、一次做一件事快切换。</li><li><strong>GPU = 千军万马</strong>：海量士兵同时干活，适合“<strong>同构小任务</strong>”的并行。</li></ul><h3 id=13-可选-mermaid-图cpu-执行-vs-gpu-并行 class=headerLink><a href=#13-%e5%8f%af%e9%80%89-mermaid-%e5%9b%becpu-%e6%89%a7%e8%a1%8c-vs-gpu-%e5%b9%b6%e8%a1%8c class=header-mark></a>1.3 可选 Mermaid 图（CPU 执行 vs GPU 并行）</h3><pre class=mermaid>flowchart LR
    subgraph CPU["CPU（顺序/少核）"]
      A1[任务1-片段A] --> A2[任务2-片段B] --> A3[任务3-片段C]
    end
    subgraph GPU["GPU（并行/多核）"]
      B1[元素1计算]:::p
      B2[元素2计算]:::p
      B3[元素3计算]:::p
      B4[元素4计算]:::p
    end
    classDef p fill:#e9f5ff,stroke:#3b82f6,stroke-width:1px;
</pre><hr><h2 id=2-张量tensor精度与量化配例子 class=headerLink><a href=#2-%e5%bc%a0%e9%87%8ftensor%e7%b2%be%e5%ba%a6%e4%b8%8e%e9%87%8f%e5%8c%96%e9%85%8d%e4%be%8b%e5%ad%90 class=header-mark></a>2. 张量（Tensor）、精度与量化（配例子）</h2><h3 id=21-张量分级 class=headerLink><a href=#21-%e5%bc%a0%e9%87%8f%e5%88%86%e7%ba%a7 class=header-mark></a>2.1 张量分级</h3><ul><li><strong>0D</strong>：标量 <code>3.14</code></li><li><strong>1D</strong>：向量 <code>[1,2,3]</code></li><li><strong>2D</strong>：矩阵（如 3×3 表）</li><li><strong>3D+</strong>：仍称张量（如 <code>batch×channel×height×width</code>）</li></ul><p><strong>图像例子</strong>：一批 32 张 224×224 RGB 图 → <code>32×3×224×224</code>（或 <code>N×H×W×C</code>，视框架而定）。</p></div><div class=post-footer><a href=/ai%E6%95%99%E7%A8%8B4/>阅读全文</a><div class=post-tags><svg class="icon" viewBox="0 0 640 512"><path d="M497.941 225.941 286.059 14.059A48 48 0 00252.118.0H48C21.49.0.0 21.49.0 48v204.118a48 48 0 0014.059 33.941l211.882 211.882c18.744 18.745 49.136 18.746 67.882.0l204.118-204.118c18.745-18.745 18.745-49.137.0-67.882zM112 160c-26.51.0-48-21.49-48-48s21.49-48 48-48 48 21.49 48 48-21.49 48-48 48zm513.941 133.823L421.823 497.941c-18.745 18.745-49.137 18.745-67.882.0l-.36-.36L527.64 323.522c16.999-16.999 26.36-39.6 26.36-63.64s-9.362-46.641-26.36-63.64L331.397.0h48.721a48 48 0 0133.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137.0 67.882z"/></svg>&nbsp;<a href=/tags/gpu/>GPU</a>,&nbsp;<a href=/tags/cuda/>CUDA</a>,&nbsp;<a href=/tags/pytorch/>PyTorch</a>,&nbsp;<a href=/tags/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/>模型训练</a>,&nbsp;<a href=/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a>,&nbsp;<a href=/tags/%E6%95%99%E7%A8%8B/>教程</a>,&nbsp;<a href=/tags/%E7%A1%AC%E4%BB%B6/>硬件</a>,&nbsp;<a href=/tags/%E5%AE%9E%E6%88%98/>实战</a></div></div></article><article class="single summary"><h1 class=single-title><a href=/ai%E6%95%99%E7%A8%8B5/>RAG系统完全指南——从零搭建本地检索增强生成系统</a></h1><div class=post-meta><span class=post-author><img class="tw-inline-block tw-max-h-4 tw-rounded-full tw-translate-y-[-2px] tw-mr-1" src=/pictures/avatar/angryCat.png alt="Finn avatar" height=16 width=16><a href=https://blog.baifan.site title=Author target=_blank rel="noopener noreferrer author" class=author>Finn</a>
</span>&nbsp;<span class=post-publish>发布于 <time datetime=2025-11-06>2025-11-06</time></span>&nbsp;<span class=post-category>收录于 </span>&nbsp;<span class=post-category>类别 <a href=/categories/ai%E6%8A%80%E6%9C%AF/><svg class="icon" viewBox="0 0 512 512"><path d="M464 128H272l-54.63-54.63c-6-6-14.14-9.37-22.63-9.37H48C21.49 64 0 85.49.0 112v288c0 26.51 21.49 48 48 48h416c26.51.0 48-21.49 48-48V176c0-26.51-21.49-48-48-48zm0 272H48V112h140.12l54.63 54.63c6 6 14.14 9.37 22.63 9.37H464v224z"/></svg>AI技术</a></span>&nbsp;<span class=post-category>和</span>&nbsp;<span class=post-series>系列 <a href><svg class="icon" viewBox="0 0 512 512"><path d="M464 32H48C21.49 32 0 53.49.0 80v352c0 26.51 21.49 48 48 48h416c26.51.0 48-21.49 48-48V80c0-26.51-21.49-48-48-48zm-6 4e2H54a6 6 0 01-6-6V86a6 6 0 016-6h404a6 6 0 016 6v340a6 6 0 01-6 6zm-42-92v24c0 6.627-5.373 12-12 12H204c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h2e2c6.627.0 12 5.373 12 12zm0-96v24c0 6.627-5.373 12-12 12H204c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h2e2c6.627.0 12 5.373 12 12zm0-96v24c0 6.627-5.373 12-12 12H204c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h2e2c6.627.0 12 5.373 12 12zm-252 12c0 19.882-16.118 36-36 36s-36-16.118-36-36 16.118-36 36-36 36 16.118 36 36zm0 96c0 19.882-16.118 36-36 36s-36-16.118-36-36 16.118-36 36-36 36 16.118 36 36zm0 96c0 19.882-16.118 36-36 36s-36-16.118-36-36 16.118-36 36-36 36 16.118 36 36z"/></svg></a></span></div><div class=content><h1 id=用-langchain--ollama--pgvector-搭建本地-rag从-0-到-1-的完整实战含-uv-依赖管理--面试指南 class=headerLink><a href=#%e7%94%a8-langchain--ollama--pgvector-%e6%90%ad%e5%bb%ba%e6%9c%ac%e5%9c%b0-rag%e4%bb%8e-0-%e5%88%b0-1-%e7%9a%84%e5%ae%8c%e6%95%b4%e5%ae%9e%e6%88%98%e5%90%ab-uv-%e4%be%9d%e8%b5%96%e7%ae%a1%e7%90%86--%e9%9d%a2%e8%af%95%e6%8c%87%e5%8d%97 class=header-mark></a>用 LangChain + Ollama + pgvector 搭建本地 RAG：从 0 到 1 的完整实战（含 uv 依赖管理 & 面试指南）</h1><blockquote><p>本文是可直接落地的 <strong>Markdown 文档</strong>。按文档自上而下执行即可从零搭建出一个本地 RAG（检索增强生成）系统，并理解关键概念与代码。所有核心脚本都附带中文注释，便于学习与面试复盘。</p></div><div class=post-footer><a href=/ai%E6%95%99%E7%A8%8B5/>阅读全文</a><div class=post-tags><svg class="icon" viewBox="0 0 640 512"><path d="M497.941 225.941 286.059 14.059A48 48 0 00252.118.0H48C21.49.0.0 21.49.0 48v204.118a48 48 0 0014.059 33.941l211.882 211.882c18.744 18.745 49.136 18.746 67.882.0l204.118-204.118c18.745-18.745 18.745-49.137.0-67.882zM112 160c-26.51.0-48-21.49-48-48s21.49-48 48-48 48 21.49 48 48-21.49 48-48 48zm513.941 133.823L421.823 497.941c-18.745 18.745-49.137 18.745-67.882.0l-.36-.36L527.64 323.522c16.999-16.999 26.36-39.6 26.36-63.64s-9.362-46.641-26.36-63.64L331.397.0h48.721a48 48 0 0133.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137.0 67.882z"/></svg>&nbsp;<a href=/tags/rag/>RAG</a>,&nbsp;<a href=/tags/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/>向量数据库</a>,&nbsp;<a href=/tags/langchain/>LangChain</a>,&nbsp;<a href=/tags/ollama/>Ollama</a>,&nbsp;<a href=/tags/postgresql/>PostgreSQL</a>,&nbsp;<a href=/tags/%E6%95%99%E7%A8%8B/>教程</a>,&nbsp;<a href=/tags/%E5%AE%9E%E6%88%98/>实战</a>,&nbsp;<a href=/tags/ai%E5%BA%94%E7%94%A8/>AI应用</a></div></div></article><article class="single summary"><h1 class=single-title><a href=/tiktok%E7%9A%84%E5%9B%B0%E5%B1%80/>当TikTok遇上特朗普：做多错多，何以关关难过关关过</a></h1><div class=post-meta><span class=post-author><img class="tw-inline-block tw-max-h-4 tw-rounded-full tw-translate-y-[-2px] tw-mr-1" src=/pictures/avatar/angryCat.png alt="Finn avatar" height=16 width=16><a href=https://blog.baifan.site title=Author target=_blank rel="noopener noreferrer author" class=author>Finn</a>
</span>&nbsp;<span class=post-publish>发布于 <time datetime=2025-11-05>2025-11-05</time></span>&nbsp;<span class=post-category>收录于 </span>&nbsp;<span class=post-category>类别 <a href=/categories/%E6%97%B6%E4%BA%8B%E5%88%86%E6%9E%90/><svg class="icon" viewBox="0 0 512 512"><path d="M464 128H272l-54.63-54.63c-6-6-14.14-9.37-22.63-9.37H48C21.49 64 0 85.49.0 112v288c0 26.51 21.49 48 48 48h416c26.51.0 48-21.49 48-48V176c0-26.51-21.49-48-48-48zm0 272H48V112h140.12l54.63 54.63c6 6 14.14 9.37 22.63 9.37H464v224z"/></svg>时事分析</a></span>&nbsp;<span class=post-category>和</span>&nbsp;<span class=post-series>系列 <a href><svg class="icon" viewBox="0 0 512 512"><path d="M464 32H48C21.49 32 0 53.49.0 80v352c0 26.51 21.49 48 48 48h416c26.51.0 48-21.49 48-48V80c0-26.51-21.49-48-48-48zm-6 4e2H54a6 6 0 01-6-6V86a6 6 0 016-6h404a6 6 0 016 6v340a6 6 0 01-6 6zm-42-92v24c0 6.627-5.373 12-12 12H204c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h2e2c6.627.0 12 5.373 12 12zm0-96v24c0 6.627-5.373 12-12 12H204c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h2e2c6.627.0 12 5.373 12 12zm0-96v24c0 6.627-5.373 12-12 12H204c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h2e2c6.627.0 12 5.373 12 12zm-252 12c0 19.882-16.118 36-36 36s-36-16.118-36-36 16.118-36 36-36 36 16.118 36 36zm0 96c0 19.882-16.118 36-36 36s-36-16.118-36-36 16.118-36 36-36 36 16.118 36 36zm0 96c0 19.882-16.118 36-36 36s-36-16.118-36-36 16.118-36 36-36 36 16.118 36 36z"/></svg></a></span></div><div class=content><h1 id=当-tiktok-遇上特朗普做多错多何以关关难过关关过 class=headerLink><a href=#%e5%bd%93-tiktok-%e9%81%87%e4%b8%8a%e7%89%b9%e6%9c%97%e6%99%ae%e5%81%9a%e5%a4%9a%e9%94%99%e5%a4%9a%e4%bd%95%e4%bb%a5%e5%85%b3%e5%85%b3%e9%9a%be%e8%bf%87%e5%85%b3%e5%85%b3%e8%bf%87 class=header-mark></a>当 TikTok 遇上特朗普：做多错多，何以关关难过关关过</h1><blockquote><p>曾被逼入绝境的 TikTok 找到了生存之道：将舞台、将掌声、将光荣最大幅度献给特朗普</p></blockquote><p>TikTok，这颗地缘政治棋盘上最受瞩目的棋子。</p></div><div class=post-footer><a href=/tiktok%E7%9A%84%E5%9B%B0%E5%B1%80/>阅读全文</a><div class=post-tags><svg class="icon" viewBox="0 0 640 512"><path d="M497.941 225.941 286.059 14.059A48 48 0 00252.118.0H48C21.49.0.0 21.49.0 48v204.118a48 48 0 0014.059 33.941l211.882 211.882c18.744 18.745 49.136 18.746 67.882.0l204.118-204.118c18.745-18.745 18.745-49.137.0-67.882zM112 160c-26.51.0-48-21.49-48-48s21.49-48 48-48 48 21.49 48 48-21.49 48-48 48zm513.941 133.823L421.823 497.941c-18.745 18.745-49.137 18.745-67.882.0l-.36-.36L527.64 323.522c16.999-16.999 26.36-39.6 26.36-63.64s-9.362-46.641-26.36-63.64L331.397.0h48.721a48 48 0 0133.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137.0 67.882z"/></svg>&nbsp;<a href=/tags/%E5%9C%B0%E7%BC%98%E6%94%BF%E6%B2%BB/>地缘政治</a>,&nbsp;<a href=/tags/%E5%95%86%E4%B8%9A%E5%88%86%E6%9E%90/>商业分析</a>,&nbsp;<a href=/tags/%E7%A4%BE%E4%BA%A4%E5%AA%92%E4%BD%93/>社交媒体</a>,&nbsp;<a href=/tags/%E4%B8%AD%E7%BE%8E%E5%85%B3%E7%B3%BB/>中美关系</a>,&nbsp;<a href=/tags/%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90/>深度分析</a>,&nbsp;<a href=/tags/%E7%A7%91%E6%8A%80%E8%A1%8C%E4%B8%9A/>科技行业</a></div></div></article><article class="single summary"><h1 class=single-title><a href=/ai%E4%B8%93%E4%B8%9A%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A%E8%A1%A8/>AI专业名词解释表：270+术语完全指南与AI技术体系词典</a></h1><div class=post-meta><span class=post-author><img class="tw-inline-block tw-max-h-4 tw-rounded-full tw-translate-y-[-2px] tw-mr-1" src=/pictures/avatar/angryCat.png alt="Finn avatar" height=16 width=16><a href=https://blog.baifan.site title=Author target=_blank rel="noopener noreferrer author" class=author>Finn</a>
</span>&nbsp;<span class=post-publish>发布于 <time datetime=2025-11-05>2025-11-05</time></span>&nbsp;<span class=post-category>收录于 </span>&nbsp;<span class=post-category>类别 <a href=/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/><svg class="icon" viewBox="0 0 512 512"><path d="M464 128H272l-54.63-54.63c-6-6-14.14-9.37-22.63-9.37H48C21.49 64 0 85.49.0 112v288c0 26.51 21.49 48 48 48h416c26.51.0 48-21.49 48-48V176c0-26.51-21.49-48-48-48zm0 272H48V112h140.12l54.63 54.63c6 6 14.14 9.37 22.63 9.37H464v224z"/></svg>学习笔记</a></span>&nbsp;<span class=post-category>和</span>&nbsp;<span class=post-series>系列 <a href><svg class="icon" viewBox="0 0 512 512"><path d="M464 32H48C21.49 32 0 53.49.0 80v352c0 26.51 21.49 48 48 48h416c26.51.0 48-21.49 48-48V80c0-26.51-21.49-48-48-48zm-6 4e2H54a6 6 0 01-6-6V86a6 6 0 016-6h404a6 6 0 016 6v340a6 6 0 01-6 6zm-42-92v24c0 6.627-5.373 12-12 12H204c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h2e2c6.627.0 12 5.373 12 12zm0-96v24c0 6.627-5.373 12-12 12H204c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h2e2c6.627.0 12 5.373 12 12zm0-96v24c0 6.627-5.373 12-12 12H204c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h2e2c6.627.0 12 5.373 12 12zm-252 12c0 19.882-16.118 36-36 36s-36-16.118-36-36 16.118-36 36-36 36 16.118 36 36zm0 96c0 19.882-16.118 36-36 36s-36-16.118-36-36 16.118-36 36-36 36 16.118 36 36zm0 96c0 19.882-16.118 36-36 36s-36-16.118-36-36 16.118-36 36-36 36 16.118 36 36z"/></svg></a></span></div><div class=content><h1 id=ai专业名词解释表 class=headerLink><a href=#ai%e4%b8%93%e4%b8%9a%e5%90%8d%e8%af%8d%e8%a7%a3%e9%87%8a%e8%a1%a8 class=header-mark></a>AI专业名词解释表</h1><p>本文档整理了AI大模型领域的核心专业术语，从基础概念到高级技术架构，帮助您系统性地理解人工智能技术体系。</p><hr><h2 id=-基础概念篇 class=headerLink><a href=#-%e5%9f%ba%e7%a1%80%e6%a6%82%e5%bf%b5%e7%af%87 class=header-mark></a>📚 基础概念篇</h2><div class=table-wrapper><table><thead><tr><th style=text-align:>名词</th><th style=text-align:>专业解释</th><th style=text-align:>通俗解释</th><th style=text-align:>举例说明</th></tr></thead><tbody><tr><td style=text-align:><strong>AGI（通用人工智能）</strong></td><td style=text-align:>Artificial General Intelligence，具备人类水平智能的AI系统</td><td style=text-align:>能像人一样思考、学习、创造的全能AI</td><td style=text-align:>一个能同时写诗、编程、做饭、聊天的机器人</td></tr><tr><td style=text-align:><strong>LLM（大语言模型）</strong></td><td style=text-align:>Large Language Model，基于海量数据训练的大型神经网络模型</td><td style=text-align:>能理解和生成人类语言的"超级大脑"</td><td style=text-align:>GPT-4、Claude、文心一言等都是LLM</td></tr><tr><td style=text-align:><strong>训练</strong></td><td style=text-align:>通过大量数据训练神经网络参数的过程</td><td style=text-align:>AI的"学习阶段"，像人读书积累知识</td><td style=text-align:>用互联网所有文本训练一个模型学会语言</td></tr><tr><td style=text-align:><strong>推理</strong></td><td style=text-align:>训练完成的模型根据输入生成输出的过程</td><td style=text-align:>AI的"应用阶段"，像人运用所学知识回答问题</td><td style=text-align:>输入问题后模型生成回答的过程</td></tr><tr><td style=text-align:><strong>Token（词元）</strong></td><td style=text-align:>模型处理文本的最小单元，通过分词算法切分的文本片段</td><td style=text-align:>AI语言的"字粒子"，模型一个一个处理</td><td style=text-align:>&ldquo;我喜欢苹果&rdquo; → [&ldquo;我&rdquo;, &ldquo;喜欢&rdquo;, &ldquo;苹果&rdquo;]</td></tr></tbody></table></div><hr><h2 id=-架构技术篇 class=headerLink><a href=#-%e6%9e%b6%e6%9e%84%e6%8a%80%e6%9c%af%e7%af%87 class=header-mark></a>🏗️ 架构技术篇</h2><div class=table-wrapper><table><thead><tr><th style=text-align:>名词</th><th style=text-align:>专业解释</th><th style=text-align:>通俗解释</th><th style=text-align:>举例说明</th></tr></thead><tbody><tr><td style=text-align:><strong>Transformer</strong></td><td style=text-align:>基于自注意力机制的深度学习架构，2017年Google提出</td><td style=text-align:>现代AI的"神经骨架"，让模型高效理解语言</td><td style=text-align:>GPT、BERT等所有大模型都基于Transformer</td></tr><tr><td style=text-align:><strong>Encoder（编码器）</strong></td><td style=text-align:>将输入序列编码为语义表示的神经网络组件</td><td style=text-align:>AI的"理解器"，把文字变成机器懂的向量</td><td style=text-align:>BERT使用Encoder做文本理解任务</td></tr><tr><td style=text-align:><strong>Decoder（解码器）</strong></td><td style=text-align:>根据上下文逐token生成输出的神经网络组件</td><td style=text-align:>AI的"写作器"，根据理解生成回答</td><td style=text-align:>GPT系列都是Decoder-only模型</td></tr><tr><td style=text-align:><strong>Self-Attention（自注意力）</strong></td><td style=text-align:>计算序列中每个元素与其他元素相关性的机制</td><td style=text-align:>AI自动"关注重点"，像人阅读时抓重点</td><td style=text-align:>&ldquo;银行"在"存钱"中关注"钱&rdquo;，在"钓鱼"中关注"河"</td></tr><tr><td style=text-align:><strong>Multi-Head Attention（多头注意力）</strong></td><td style=text-align:>并行多个自注意力机制，捕获不同类型的依赖关系</td><td style=text-align:>AI从多个角度同时理解文本</td><td style=text-align:>一个头关注语法，另一个头关注语义</td></tr><tr><td style=text-align:><strong>Positional Encoding（位置编码）</strong></td><td style=text-align:>为每个token添加位置信息的向量表示</td><td style=text-align:>让模型知道"谁在前谁在后"</td><td style=text-align:>&ldquo;我爱你"与"你爱我"意义不同</td></tr><tr><td style=text-align:><strong>Query（查询向量）</strong></td><td style=text-align:>主动查询相关信息的向量，表示当前词需要什么信息</td><td style=text-align:>&ldquo;我要找什么"的数字表达</td><td style=text-align:>&ldquo;苹果"查询相关的味道、颜色等属性</td></tr><tr><td style=text-align:><strong>Key（键向量）</strong></td><td style=text-align:>被查询信息的标识向量，表示每个词能提供什么信息</td><td style=text-align:>&ldquo;我能提供什么"的标签</td><td style=text-align:>&ldquo;甜"作为味道特征的Key，等待被查询</td></tr><tr><td style=text-align:><strong>Value（值向量）</strong></td><td style=text-align:>实际内容的表示向量，包含词的真实语义信息</td><td style=text-align:>&ldquo;我的具体内容"的数值化</td><td style=text-align:>&ldquo;甜"的实际语义表示[0.8, 0.2, -0.1]</td></tr><tr><td style=text-align:><strong>Attention Weight（注意力权重）</strong></td><td style=text-align:>表示关注程度的重要性分数，通常通过softmax归一化</td><td style=text-align:>&ldquo;关注程度"的数值化</td><td style=text-align:>0.8表示强烈关注，0.1表示弱关注，所有权重和为1</td></tr><tr><td style=text-align:><strong>Cross-Attention（交叉注意力）</strong></td><td style=text-align:>不同序列间的注意力机制，Query来自一个序列，Key/Value来自另一个序列</td><td style=text-align:>跨模态信息交互</td><td style=text-align:>图文匹配中文字Query关注图像Key/Value</td></tr><tr><td style=text-align:><strong>Causal Attention（因果注意力）</strong></td><td style=text-align:>只能关注当前位置及之前内容的注意力机制，防止未来信息泄露</td><td style=text-align:>&ldquo;只能向前看"的注意力</td><td style=text-align:>GPT生成时第5个词只能看前4个词</td></tr><tr><td style=text-align:><strong>Softmax Function</strong></td><td style=text-align:>将任意实数向量转换为概率分布的激活函数</td><td style=text-align:>转换为"重要性百分比&rdquo;</td><td style=text-align:><code>[2,1,0] → [0.67,0.24,0.09]</code>，保持相对大小关系</td></tr></tbody></table></div><hr><h2 id=-数学表示篇 class=headerLink><a href=#-%e6%95%b0%e5%ad%a6%e8%a1%a8%e7%a4%ba%e7%af%87 class=header-mark></a>🔢 数学表示篇</h2><div class=table-wrapper><table><thead><tr><th style=text-align:>名词</th><th style=text-align:>专业解释</th><th style=text-align:>通俗解释</th><th style=text-align:>举例说明</th></tr></thead><tbody><tr><td style=text-align:><strong>Vector（向量）</strong></td><td style=text-align:>具有大小和方向的数学对象，一组有序数字</td><td style=text-align:>事物的"数字身份证&rdquo;，用数字描述特征</td><td style=text-align:><code>[25, 180, 70]</code>可表示一个人的年龄、身高、体重</td></tr><tr><td style=text-align:><strong>Embedding（嵌入）</strong></td><td style=text-align:>将离散符号映射到连续向量空间的技术</td><td style=text-align:>把文字变成"数字坐标&rdquo;</td><td style=text-align:><code>"国王"→[0.25, -0.12, 0.78, ...]</code></td></tr><tr><td style=text-align:><strong>Query / Key / Value</strong></td><td style=text-align:>自注意力机制中的三个核心向量矩阵，分别代表查询需求、标识信息、实际内容</td><td style=text-align:>Query=我要什么，Key=我能提供什么，Value=我的具体内容</td><td style=text-align:><code>Query=[0.1,0.2]</code>查询味道，<code>Key=[0.8,0.1]</code>标识甜味，<code>Value=[0.9,0.05]</code>甜味的实际表示</td></tr><tr><td style=text-align:><strong>Feed-Forward Network（前馈网络）</strong></td><td style=text-align:>对每个位置独立进行非线性变换</td><td style=text-align:>深化每个词的理解</td><td style=text-align:>&ldquo;春天"进一步联想到"温暖、生长&rdquo;</td></tr><tr><td style=text-align:><strong>Layer Normalization（层归一化）</strong></td><td style=text-align:>标准化层输入</td><td style=text-align:>训练"稳定器&rdquo;</td><td style=text-align:>防止梯度爆炸或发散</td></tr><tr><td style=text-align:><strong>Residual Connection（残差连接）</strong></td><td style=text-align:>跨层连接，保留原始信息</td><td style=text-align:>信息"直通车&rdquo;，防止丢失</td><td style=text-align:>类似捷径路径避免深层网络退化</td></tr></tbody></table></div><hr><h2 id=-处理流程篇 class=headerLink><a href=#-%e5%a4%84%e7%90%86%e6%b5%81%e7%a8%8b%e7%af%87 class=header-mark></a>🔄 处理流程篇</h2><div class=table-wrapper><table><thead><tr><th style=text-align:>名词</th><th style=text-align:>专业解释</th><th style=text-align:>通俗解释</th><th style=text-align:>举例说明</th></tr></thead><tbody><tr><td style=text-align:><strong>Tokenizer（分词器）</strong></td><td style=text-align:>将文本转换为token序列</td><td style=text-align:>&ldquo;文字切菜刀&rdquo;</td><td style=text-align:><code>"Hello world" → ["Hello", " world"]</code></td></tr><tr><td style=text-align:><strong>Context Window（上下文窗口）</strong></td><td style=text-align:>模型能处理的最大token数量限制</td><td style=text-align:>AI的"记忆力上限&rdquo;</td><td style=text-align:>GPT-4有128K上下文</td></tr><tr><td style=text-align:><strong>Decoding（解码）</strong></td><td style=text-align:>根据概率分布逐token生成文本</td><td style=text-align:>AI"写字过程&rdquo;</td><td style=text-align:>从最可能的词开始生成</td></tr><tr><td style=text-align:><strong>Temperature（温度参数）</strong></td><td style=text-align:>控制生成随机性的参数</td><td style=text-align:>&ldquo;创意调节器&rdquo;</td><td style=text-align:>高温更有创意，低温更稳健</td></tr><tr><td style=text-align:><strong>Top-p采样</strong></td><td style=text-align:>基于累积概率的采样策略</td><td style=text-align:>&ldquo;精华筛选器&rdquo;</td><td style=text-align:>只考虑累计概率达到90%的候选词</td></tr><tr><td style=text-align:><strong>Max Tokens（最大令牌数）</strong></td><td style=text-align:>限制生成输出长度</td><td style=text-align:>&ldquo;字数限制器&rdquo;</td><td style=text-align:>防止AI回答过长</td></tr></tbody></table></div><hr><h2 id=-工程实践篇 class=headerLink><a href=#-%e5%b7%a5%e7%a8%8b%e5%ae%9e%e8%b7%b5%e7%af%87 class=header-mark></a>🛠️ 工程实践篇</h2><div class=table-wrapper><table><thead><tr><th style=text-align:>名词</th><th style=text-align:>专业解释</th><th style=text-align:>通俗解释</th><th style=text-align:>举例说明</th></tr></thead><tbody><tr><td style=text-align:><strong>RAG（检索增强生成）</strong></td><td style=text-align:>结合检索和生成的AI方法</td><td style=text-align:>&ldquo;开卷考试"式AI</td><td style=text-align:>先查资料再回答问题</td></tr><tr><td style=text-align:><strong>Prompt Engineering（提示工程）</strong></td><td style=text-align:>设计优化提示词的技术</td><td style=text-align:>&ldquo;说话艺术&rdquo;</td><td style=text-align:>让AI更好理解需求</td></tr><tr><td style=text-align:><strong>Fine-tuning（微调）</strong></td><td style=text-align:>在预训练模型上进行特定任务训练</td><td style=text-align:>&ldquo;定向培训&rdquo;</td><td style=text-align:>让通用模型变成医疗助手</td></tr><tr><td style=text-align:><strong>BPE（字节对编码）</strong></td><td style=text-align:>一种常见分词算法</td><td style=text-align:>&ldquo;文字压缩术&rdquo;</td><td style=text-align:><code>"unhappiness" → ["un","happi","ness"]</code></td></tr><tr><td style=text-align:><strong>Detokenization（反分词）</strong></td><td style=text-align:>将token序列还原为可读文本</td><td style=text-align:>&ldquo;拼字还原&rdquo;</td><td style=text-align:><code>["我","喜欢","苹果"]→"我喜欢苹果"</code></td></tr><tr><td style=text-align:><strong>Streaming（流式输出）</strong></td><td style=text-align:>逐token实时生成输出</td><td style=text-align:>&ldquo;打字机效果&rdquo;</td><td style=text-align:>聊天机器人边输出边思考</td></tr></tbody></table></div><hr><h2 id=-传统模型对比篇 class=headerLink><a href=#-%e4%bc%a0%e7%bb%9f%e6%a8%a1%e5%9e%8b%e5%af%b9%e6%af%94%e7%af%87 class=header-mark></a>🧠 传统模型对比篇</h2><div class=table-wrapper><table><thead><tr><th style=text-align:>名词</th><th style=text-align:>专业解释</th><th style=text-align:>通俗解释</th><th style=text-align:>举例说明</th></tr></thead><tbody><tr><td style=text-align:><strong>RNN（循环神经网络）</strong></td><td style=text-align:>逐步处理序列数据的神经网络</td><td style=text-align:>&ldquo;逐字阅读AI&rdquo;</td><td style=text-align:>翻译<code>"我爱你"</code>逐词处理</td></tr><tr><td style=text-align:><strong>LSTM（长短期记忆网络）</strong></td><td style=text-align:>改进型RNN，解决长期依赖问题</td><td style=text-align:>&ldquo;记忆力更强&rdquo;</td><td style=text-align:>能记住开头内容</td></tr><tr><td style=text-align:><strong>CNN（卷积神经网络）</strong></td><td style=text-align:>擅长处理图像模式的神经网络</td><td style=text-align:>&ldquo;图像专家&rdquo;</td><td style=text-align:>识别猫狗人脸</td></tr><tr><td style=text-align:><strong>Encoder-Decoder架构</strong></td><td style=text-align:>同时包含理解与生成模块的模型</td><td style=text-align:>&ldquo;全能型AI&rdquo;</td><td style=text-align:>机器翻译模型</td></tr></tbody></table></div><hr><h2 id=-应用场景篇 class=headerLink><a href=#-%e5%ba%94%e7%94%a8%e5%9c%ba%e6%99%af%e7%af%87 class=header-mark></a>📊 应用场景篇</h2><div class=table-wrapper><table><thead><tr><th style=text-align:>名词</th><th style=text-align:>专业解释</th><th style=text-align:>通俗解释</th><th style=text-align:>举例说明</th></tr></thead><tbody><tr><td style=text-align:><strong>对话产品</strong></td><td style=text-align:>面向用户的AI应用接口</td><td style=text-align:>&ldquo;AI聊天壳&rdquo;</td><td style=text-align:>ChatGPT、Claude</td></tr><tr><td style=text-align:><strong>API调用</strong></td><td style=text-align:>程序间通信接口</td><td style=text-align:>&ldquo;AI电话线&rdquo;</td><td style=text-align:>程序调用OpenAI API</td></tr><tr><td style=text-align:><strong>上下文管理</strong></td><td style=text-align:>维护对话历史的技术</td><td style=text-align:>&ldquo;AI记忆力&rdquo;</td><td style=text-align:>聊天机器人记住你说过的话</td></tr><tr><td style=text-align:><strong>多轮对话</strong></td><td style=text-align:>连续人机交互模式</td><td style=text-align:>&ldquo;连续聊天&rdquo;</td><td style=text-align:>先问天气，再问穿衣</td></tr><tr><td style=text-align:><strong>工具调用（Function Calling）</strong></td><td style=text-align:>模型可调用外部API执行任务</td><td style=text-align:>&ldquo;AI动手能力&rdquo;</td><td style=text-align:>AI自动查天气或搜索资料</td></tr></tbody></table></div><hr><h2 id=-模型优化与训练技巧篇 class=headerLink><a href=#-%e6%a8%a1%e5%9e%8b%e4%bc%98%e5%8c%96%e4%b8%8e%e8%ae%ad%e7%bb%83%e6%8a%80%e5%b7%a7%e7%af%87 class=header-mark></a>🧩 模型优化与训练技巧篇</h2><div class=table-wrapper><table><thead><tr><th style=text-align:>名词</th><th style=text-align:>专业解释</th><th style=text-align:>通俗解释</th><th style=text-align:>举例说明</th></tr></thead><tbody><tr><td style=text-align:><strong>LoRA（低秩适配）</strong></td><td style=text-align:>通过低秩矩阵微调模型参数</td><td style=text-align:>&ldquo;轻量级微调&rdquo;</td><td style=text-align:>让LLM快速适应新领域</td></tr><tr><td style=text-align:><strong>Quantization（量化）</strong></td><td style=text-align:>用低精度表示模型参数</td><td style=text-align:>&ldquo;模型瘦身&rdquo;</td><td style=text-align:>FP32→INT8加速推理</td></tr><tr><td style=text-align:><strong>Pruning（剪枝）</strong></td><td style=text-align:>删除冗余神经元或连接</td><td style=text-align:>&ldquo;修枝整形&rdquo;</td><td style=text-align:>去除无效参数</td></tr><tr><td style=text-align:><strong>Distillation（知识蒸馏）</strong></td><td style=text-align:>用大模型指导小模型学习</td><td style=text-align:>&ldquo;老师带学生&rdquo;</td><td style=text-align:>GPT-4教小模型</td></tr><tr><td style=text-align:><strong>Checkpoint（检查点）</strong></td><td style=text-align:>模型训练中保存的中间状态</td><td style=text-align:>&ldquo;训练存档点&rdquo;</td><td style=text-align:>防止断电丢失进度</td></tr></tbody></table></div><hr><h2 id=-向量检索与知识集成篇 class=headerLink><a href=#-%e5%90%91%e9%87%8f%e6%a3%80%e7%b4%a2%e4%b8%8e%e7%9f%a5%e8%af%86%e9%9b%86%e6%88%90%e7%af%87 class=header-mark></a>🔍 向量检索与知识集成篇</h2><div class=table-wrapper><table><thead><tr><th style=text-align:>名词</th><th style=text-align:>专业解释</th><th style=text-align:>通俗解释</th><th style=text-align:>举例说明</th></tr></thead><tbody><tr><td style=text-align:><strong>Embedding Model（向量模型）</strong></td><td style=text-align:>将文本转为语义向量的模型</td><td style=text-align:>&ldquo;语义坐标机&rdquo;</td><td style=text-align:>text-embedding-3-large</td></tr><tr><td style=text-align:><strong>Vector Database（向量数据库）</strong></td><td style=text-align:>支持向量检索的数据库</td><td style=text-align:>&ldquo;语义仓库&rdquo;</td><td style=text-align:>Milvus、Pinecone、FAISS</td></tr><tr><td style=text-align:><strong>Cosine Similarity（余弦相似度）</strong></td><td style=text-align:>衡量两个向量方向相似度</td><td style=text-align:>&ldquo;语义相似度计&rdquo;</td><td style=text-align:><code>"猫在睡觉"≈"猫咪休息中"</code></td></tr><tr><td style=text-align:><strong>Knowledge Graph（知识图谱）</strong></td><td style=text-align:>用节点和关系存储知识结构</td><td style=text-align:>&ldquo;知识地图&rdquo;</td><td style=text-align:><code>"苹果→是→水果"</code></td></tr><tr><td style=text-align:><strong>Hybrid Search（混合检索）</strong></td><td style=text-align:>结合语义检索与关键词匹配</td><td style=text-align:>&ldquo;双保险搜索&rdquo;</td><td style=text-align:>同时检索<code>"猫"</code>和<code>"宠物动物"</code></td></tr></tbody></table></div><hr><h2 id=-多模态与智能体篇 class=headerLink><a href=#-%e5%a4%9a%e6%a8%a1%e6%80%81%e4%b8%8e%e6%99%ba%e8%83%bd%e4%bd%93%e7%af%87 class=header-mark></a>🧩 多模态与智能体篇</h2><div class=table-wrapper><table><thead><tr><th style=text-align:>名词</th><th style=text-align:>专业解释</th><th style=text-align:>通俗解释</th><th style=text-align:>举例说明</th></tr></thead><tbody><tr><td style=text-align:><strong>Multimodal Model（多模态模型）</strong></td><td style=text-align:>同时处理文本、图像、音频等模态</td><td style=text-align:>&ldquo;全感官AI&rdquo;</td><td style=text-align:>GPT-4V、Gemini</td></tr><tr><td style=text-align:><strong>VLM（视觉语言模型）</strong></td><td style=text-align:>Vision-Language Model</td><td style=text-align:>&ldquo;会看图的AI&rdquo;</td><td style=text-align:>看图问答AI</td></tr><tr><td style=text-align:><strong>Speech Recognition（语音识别）</strong></td><td style=text-align:>将语音转文字</td><td style=text-align:>&ldquo;听写AI&rdquo;</td><td style=text-align:>语音输入法</td></tr><tr><td style=text-align:><strong>TTS（文本转语音）</strong></td><td style=text-align:>将文字转语音</td><td style=text-align:>&ldquo;AI播音员&rdquo;</td><td style=text-align:>AI读出回答</td></tr><tr><td style=text-align:><strong>AI Agent（智能体）</strong></td><td style=text-align:>具备自主行动与决策能力的AI</td><td style=text-align:>&ldquo;能动的AI助手&rdquo;</td><td style=text-align:>Devin、AutoGPT</td></tr></tbody></table></div><hr><h2 id=-模型评估与安全篇 class=headerLink><a href=#-%e6%a8%a1%e5%9e%8b%e8%af%84%e4%bc%b0%e4%b8%8e%e5%ae%89%e5%85%a8%e7%af%87 class=header-mark></a>⚙️ 模型评估与安全篇</h2><div class=table-wrapper><table><thead><tr><th style=text-align:>名词</th><th style=text-align:>专业解释</th><th style=text-align:>通俗解释</th><th style=text-align:>举例说明</th></tr></thead><tbody><tr><td style=text-align:><strong>Hallucination（幻觉）</strong></td><td style=text-align:>模型生成虚假信息</td><td style=text-align:>&ldquo;一本正经胡说八道&rdquo;</td><td style=text-align:>编造论文或事实</td></tr><tr><td style=text-align:><strong>Alignment（对齐）</strong></td><td style=text-align:>模型与人类价值观对齐</td><td style=text-align:>&ldquo;价值观调教&rdquo;</td><td style=text-align:>RLHF调教模型</td></tr><tr><td style=text-align:><strong>RLHF（人类反馈强化学习）</strong></td><td style=text-align:>用人类偏好优化模型</td><td style=text-align:>&ldquo;人教AI说话&rdquo;</td><td style=text-align:>ChatGPT的训练方式</td></tr><tr><td style=text-align:><strong>Red Teaming（红队测试）</strong></td><td style=text-align:>对抗性测试模型安全</td><td style=text-align:>&ldquo;安全渗透测试&rdquo;</td><td style=text-align:>测试模型是否泄密</td></tr><tr><td style=text-align:><strong>Bias（偏差）</strong></td><td style=text-align:>模型输出的系统性偏见</td><td style=text-align:>&ldquo;AI偏心&rdquo;</td><td style=text-align:>对性别或语言偏好</td></tr></tbody></table></div><hr><h2 id=-新兴趋势与未来方向篇 class=headerLink><a href=#-%e6%96%b0%e5%85%b4%e8%b6%8b%e5%8a%bf%e4%b8%8e%e6%9c%aa%e6%9d%a5%e6%96%b9%e5%90%91%e7%af%87 class=header-mark></a>🧰 新兴趋势与未来方向篇</h2><div class=table-wrapper><table><thead><tr><th style=text-align:>名词</th><th style=text-align:>专业解释</th><th style=text-align:>通俗解释</th><th style=text-align:>举例说明</th></tr></thead><tbody><tr><td style=text-align:><strong>Mixture of Experts（专家混合）</strong></td><td style=text-align:>包含多个子模型动态激活结构</td><td style=text-align:>&ldquo;专家组AI&rdquo;</td><td style=text-align:><code>Gemini 1.5 Pro</code>架构</td></tr><tr><td style=text-align:><strong>Context Compression（上下文压缩）</strong></td><td style=text-align:>压缩历史对话节省token</td><td style=text-align:>&ldquo;记忆压缩&rdquo;</td><td style=text-align:>长对话摘要</td></tr><tr><td style=text-align:><strong>Memory-Augmented Model（记忆增强模型）</strong></td><td style=text-align:>结合长期记忆机制的AI</td><td style=text-align:>&ldquo;有记忆的AI&rdquo;</td><td style=text-align:><code>ChatGPT</code>长期记忆功能</td></tr><tr><td style=text-align:><strong>Autonomous Agent（自主智能体）</strong></td><td style=text-align:>能自我规划执行任务的AI</td><td style=text-align:>&ldquo;自理AI&rdquo;</td><td style=text-align:><code>AutoGPT</code>、<code>Devin</code></td></tr><tr><td style=text-align:><strong>Synthetic Data（合成数据）</strong></td><td style=text-align:>由AI生成的虚拟训练数据</td><td style=text-align:>&ldquo;AI自制教材&rdquo;</td><td style=text-align:>用AI扩充训练集</td></tr></tbody></table></div><hr><h2 id=-学习建议 class=headerLink><a href=#-%e5%ad%a6%e4%b9%a0%e5%bb%ba%e8%ae%ae class=header-mark></a>💡 学习建议</h2><h3 id=-核心概念掌握优先级 class=headerLink><a href=#-%e6%a0%b8%e5%bf%83%e6%a6%82%e5%bf%b5%e6%8e%8c%e6%8f%a1%e4%bc%98%e5%85%88%e7%ba%a7 class=header-mark></a>🎯 核心概念掌握优先级</h3><ol><li><strong>入门级（必掌握）</strong>：Token、Embedding、Transformer、LLM</li><li><strong>进阶级（重要）</strong>：Self-Attention、RAG、Context Window</li><li><strong>高级（可选）</strong>：LoRA、Mixture of Experts、Red Teaming</li></ol><h3 id=-学习路径建议 class=headerLink><a href=#-%e5%ad%a6%e4%b9%a0%e8%b7%af%e5%be%84%e5%bb%ba%e8%ae%ae class=header-mark></a>📖 学习路径建议</h3><ol><li><strong>理解基本原理</strong>：Token是什么，为什么需要向量表示</li><li><strong>掌握核心架构</strong>：Transformer的Encoder-Decoder结构</li><li><strong>实践应用技巧</strong>：Prompt工程与RAG结合</li><li><strong>深入技术细节</strong>：注意力机制与对齐训练</li></ol><h3 id=-概念关联图 class=headerLink><a href=#-%e6%a6%82%e5%bf%b5%e5%85%b3%e8%81%94%e5%9b%be class=header-mark></a>🔗 概念关联图</h3><div class="code-block highlight is-open show-line-numbers tw-group tw-my-2"><div class="tw-flex
tw-flex-row
tw-flex-1
tw-justify-between
tw-w-full tw-bg-bgColor-secondary"><button class="code-block-button
tw-mx-2
tw-flex
tw-flex-row
tw-flex-1" aria-hidden=true><div class="group-[.is-open]:tw-rotate-90 tw-transition-[transform] tw-duration-500 tw-ease-in-out print:!tw-hidden tw-w-min tw-h-min tw-my-1 tw-mx-1"><svg class="icon" viewBox="0 0 320 512"><path d="M285.476 272.971 91.132 467.314c-9.373 9.373-24.569 9.373-33.941.0l-22.667-22.667c-9.357-9.357-9.375-24.522-.04-33.901L188.505 256 34.484 101.255c-9.335-9.379-9.317-24.544.04-33.901l22.667-22.667c9.373-9.373 24.569-9.373 33.941.0L285.475 239.03c9.373 9.372 9.373 24.568.001 33.941z"/></svg></div><p class="tw-select-none !tw-my-1">text</p></div><div class=post-footer><a href=/ai%E4%B8%93%E4%B8%9A%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A%E8%A1%A8/>阅读全文</a><div class=post-tags><svg class="icon" viewBox="0 0 640 512"><path d="M497.941 225.941 286.059 14.059A48 48 0 00252.118.0H48C21.49.0.0 21.49.0 48v204.118a48 48 0 0014.059 33.941l211.882 211.882c18.744 18.745 49.136 18.746 67.882.0l204.118-204.118c18.745-18.745 18.745-49.137.0-67.882zM112 160c-26.51.0-48-21.49-48-48s21.49-48 48-48 48 21.49 48 48-21.49 48-48 48zm513.941 133.823L421.823 497.941c-18.745 18.745-49.137 18.745-67.882.0l-.36-.36L527.64 323.522c16.999-16.999 26.36-39.6 26.36-63.64s-9.362-46.641-26.36-63.64L331.397.0h48.721a48 48 0 0133.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137.0 67.882z"/></svg>&nbsp;<a href=/tags/ai/>AI</a>,&nbsp;<a href=/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/>大模型</a>,&nbsp;<a href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a>,&nbsp;<a href=/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a>,&nbsp;<a href=/tags/%E7%AC%94%E8%AE%B0/>笔记</a>,&nbsp;<a href=/tags/%E6%9C%AF%E8%AF%AD/>术语</a>,&nbsp;<a href=/tags/%E8%AF%8D%E5%85%B8/>词典</a>,&nbsp;<a href=/tags/%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/>学习资料</a></div></div></article><article class="single summary"><h1 class=single-title><a href=/ai%E6%95%99%E7%A8%8B3/>Prompt Engineering完全指南：从提示工程到上下文工程的实战教程</a></h1><div class=post-meta><span class=post-author><img class="tw-inline-block tw-max-h-4 tw-rounded-full tw-translate-y-[-2px] tw-mr-1" src=/pictures/avatar/angryCat.png alt="Finn avatar" height=16 width=16><a href=https://blog.baifan.site title=Author target=_blank rel="noopener noreferrer author" class=author>Finn</a>
</span>&nbsp;<span class=post-publish>发布于 <time datetime=2025-11-05>2025-11-05</time></span>&nbsp;<span class=post-category>收录于 </span>&nbsp;<span class=post-category>类别 <a href=/categories/ai%E6%8A%80%E6%9C%AF/><svg class="icon" viewBox="0 0 512 512"><path d="M464 128H272l-54.63-54.63c-6-6-14.14-9.37-22.63-9.37H48C21.49 64 0 85.49.0 112v288c0 26.51 21.49 48 48 48h416c26.51.0 48-21.49 48-48V176c0-26.51-21.49-48-48-48zm0 272H48V112h140.12l54.63 54.63c6 6 14.14 9.37 22.63 9.37H464v224z"/></svg>AI技术</a></span>&nbsp;<span class=post-category>和</span>&nbsp;<span class=post-series>系列 <a href><svg class="icon" viewBox="0 0 512 512"><path d="M464 32H48C21.49 32 0 53.49.0 80v352c0 26.51 21.49 48 48 48h416c26.51.0 48-21.49 48-48V80c0-26.51-21.49-48-48-48zm-6 4e2H54a6 6 0 01-6-6V86a6 6 0 016-6h404a6 6 0 016 6v340a6 6 0 01-6 6zm-42-92v24c0 6.627-5.373 12-12 12H204c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h2e2c6.627.0 12 5.373 12 12zm0-96v24c0 6.627-5.373 12-12 12H204c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h2e2c6.627.0 12 5.373 12 12zm0-96v24c0 6.627-5.373 12-12 12H204c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h2e2c6.627.0 12 5.373 12 12zm-252 12c0 19.882-16.118 36-36 36s-36-16.118-36-36 16.118-36 36-36 36 16.118 36 36zm0 96c0 19.882-16.118 36-36 36s-36-16.118-36-36 16.118-36 36-36 36 16.118 36 36zm0 96c0 19.882-16.118 36-36 36s-36-16.118-36-36 16.118-36 36-36 36 16.118 36 36z"/></svg></a></span></div><div class=content><h1 id=ai-教程prompt-engineering class=headerLink><a href=#ai-%e6%95%99%e7%a8%8bprompt-engineering class=header-mark></a>AI 教程：Prompt Engineering</h1><p>提示工程主要关注提示词的设计、优化与策略制定，致力于帮助用户更高效地调动大语言模型的能力，进而推动其在各类实际场景和研究领域中的应用。</p></div><div class=post-footer><a href=/ai%E6%95%99%E7%A8%8B3/>阅读全文</a><div class=post-tags><svg class="icon" viewBox="0 0 640 512"><path d="M497.941 225.941 286.059 14.059A48 48 0 00252.118.0H48C21.49.0.0 21.49.0 48v204.118a48 48 0 0014.059 33.941l211.882 211.882c18.744 18.745 49.136 18.746 67.882.0l204.118-204.118c18.745-18.745 18.745-49.137.0-67.882zM112 160c-26.51.0-48-21.49-48-48s21.49-48 48-48 48 21.49 48 48-21.49 48-48 48zm513.941 133.823L421.823 497.941c-18.745 18.745-49.137 18.745-67.882.0l-.36-.36L527.64 323.522c16.999-16.999 26.36-39.6 26.36-63.64s-9.362-46.641-26.36-63.64L331.397.0h48.721a48 48 0 0133.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137.0 67.882z"/></svg>&nbsp;<a href=/tags/prompt%E5%B7%A5%E7%A8%8B/>Prompt工程</a>,&nbsp;<a href=/tags/rag/>RAG</a>,&nbsp;<a href=/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/>大模型</a>,&nbsp;<a href=/tags/ai%E5%BA%94%E7%94%A8/>AI应用</a>,&nbsp;<a href=/tags/%E6%95%99%E7%A8%8B/>教程</a>,&nbsp;<a href=/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a>,&nbsp;<a href=/tags/llm/>LLM</a></div></div></article><article class="single summary"><h1 class=single-title><a href=/ai%E6%95%99%E7%A8%8B2/>Transformer架构深度解析：注意力机制与AI大模型的核心技术</a></h1><div class=post-meta><span class=post-author><img class="tw-inline-block tw-max-h-4 tw-rounded-full tw-translate-y-[-2px] tw-mr-1" src=/pictures/avatar/angryCat.png alt="Finn avatar" height=16 width=16><a href=https://blog.baifan.site title=Author target=_blank rel="noopener noreferrer author" class=author>Finn</a>
</span>&nbsp;<span class=post-publish>发布于 <time datetime=2025-11-05>2025-11-05</time></span>&nbsp;<span class=post-category>收录于 </span>&nbsp;<span class=post-category>类别 <a href=/categories/ai%E6%8A%80%E6%9C%AF/><svg class="icon" viewBox="0 0 512 512"><path d="M464 128H272l-54.63-54.63c-6-6-14.14-9.37-22.63-9.37H48C21.49 64 0 85.49.0 112v288c0 26.51 21.49 48 48 48h416c26.51.0 48-21.49 48-48V176c0-26.51-21.49-48-48-48zm0 272H48V112h140.12l54.63 54.63c6 6 14.14 9.37 22.63 9.37H464v224z"/></svg>AI技术</a></span>&nbsp;<span class=post-category>和</span>&nbsp;<span class=post-series>系列 <a href><svg class="icon" viewBox="0 0 512 512"><path d="M464 32H48C21.49 32 0 53.49.0 80v352c0 26.51 21.49 48 48 48h416c26.51.0 48-21.49 48-48V80c0-26.51-21.49-48-48-48zm-6 4e2H54a6 6 0 01-6-6V86a6 6 0 016-6h404a6 6 0 016 6v340a6 6 0 01-6 6zm-42-92v24c0 6.627-5.373 12-12 12H204c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h2e2c6.627.0 12 5.373 12 12zm0-96v24c0 6.627-5.373 12-12 12H204c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h2e2c6.627.0 12 5.373 12 12zm0-96v24c0 6.627-5.373 12-12 12H204c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h2e2c6.627.0 12 5.373 12 12zm-252 12c0 19.882-16.118 36-36 36s-36-16.118-36-36 16.118-36 36-36 36 16.118 36 36zm0 96c0 19.882-16.118 36-36 36s-36-16.118-36-36 16.118-36 36-36 36 16.118 36 36zm0 96c0 19.882-16.118 36-36 36s-36-16.118-36-36 16.118-36 36-36 36 16.118 36 36z"/></svg></a></span></div><div class=content><h1 id=ai-教程---transformer class=headerLink><a href=#ai-%e6%95%99%e7%a8%8b---transformer class=header-mark></a>AI 教程 - Transformer</h1><h2 id=-一transformer-是什么 class=headerLink><a href=#-%e4%b8%80transformer-%e6%98%af%e4%bb%80%e4%b9%88 class=header-mark></a>🧩 一、Transformer 是什么？</h2><blockquote><p><strong>Transformer 是一种深度学习架构，用来处理序列（例如文字、语音、代码等）信息。</strong></p></blockquote><p>它最早由 Google 在 2017 年的论文《Attention Is All You Need（注意力机制就是全部）》中提出。</p></div><div class=post-footer><a href=/ai%E6%95%99%E7%A8%8B2/>阅读全文</a><div class=post-tags><svg class="icon" viewBox="0 0 640 512"><path d="M497.941 225.941 286.059 14.059A48 48 0 00252.118.0H48C21.49.0.0 21.49.0 48v204.118a48 48 0 0014.059 33.941l211.882 211.882c18.744 18.745 49.136 18.746 67.882.0l204.118-204.118c18.745-18.745 18.745-49.137.0-67.882zM112 160c-26.51.0-48-21.49-48-48s21.49-48 48-48 48 21.49 48 48-21.49 48-48 48zm513.941 133.823L421.823 497.941c-18.745 18.745-49.137 18.745-67.882.0l-.36-.36L527.64 323.522c16.999-16.999 26.36-39.6 26.36-63.64s-9.362-46.641-26.36-63.64L331.397.0h48.721a48 48 0 0133.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137.0 67.882z"/></svg>&nbsp;<a href=/tags/transformer/>Transformer</a>,&nbsp;<a href=/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/>注意力机制</a>,&nbsp;<a href=/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a>,&nbsp;<a href=/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/>大模型</a>,&nbsp;<a href=/tags/%E6%95%99%E7%A8%8B/>教程</a>,&nbsp;<a href=/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/>神经网络</a>,&nbsp;<a href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></div></div></article><ul class=pagination><li class=page-item><span class=page-link><a href=/>1</a></span></li><li class=page-item><span class=page-link><a href=/page/2/>2</a></span></li><li class="page-item active"><span class=page-link><a href=/page/3/>3</a></span></li><li class=page-item><span class=page-link><a href=/page/4/>4</a></span></li></ul></div></main><footer class=footer><div class=footer-container><div class=footer-line><svg class="icon" viewBox="0 0 512 512"><path d="M256 8C119.033 8 8 119.033 8 256s111.033 248 248 248 248-111.033 248-248S392.967 8 256 8zm0 448c-110.532.0-2e2-89.451-2e2-2e2.0-110.531 89.451-2e2 2e2-2e2 110.532.0 2e2 89.451 2e2 2e2.0 110.532-89.451 2e2-2e2 2e2zm107.351-101.064c-9.614 9.712-45.53 41.396-104.065 41.396-82.43.0-140.484-61.425-140.484-141.567.0-79.152 60.275-139.401 139.762-139.401 55.531.0 88.738 26.62 97.593 34.779a11.965 11.965.0 011.936 15.322l-18.155 28.113c-3.841 5.95-11.966 7.282-17.499 2.921-8.595-6.776-31.814-22.538-61.708-22.538-48.303.0-77.916 35.33-77.916 80.082.0 41.589 26.888 83.692 78.277 83.692 32.657.0 56.843-19.039 65.726-27.225 5.27-4.857 13.596-4.039 17.82 1.738l19.865 27.17a11.947 11.947.0 01-1.152 15.518z"/></svg>2025<span class=author>&nbsp;<a href=https://blog.baifan.site target=_blank rel="noopener noreferrer">Finn</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class=footer-line></div><div class=footer-line></div></div><script>"serviceWorker"in navigator&&(navigator.serviceWorker.register("/sw.min.js",{scope:"/"}).then(function(){}),navigator.serviceWorker.ready.then(function(){}))</script></footer><div class="print:!tw-hidden tw-flex tw-flex-col tw-fixed tw-right-4 tw-bottom-4 tw-gap-2"><a href=#back-to-top id=back-to-top-button class="tw-transition-opacity tw-opacity-0 tw-block tw-bg-bgColor-secondary tw-rounded-full" style=padding:.6rem;line-height:1.3rem;font-size:1rem title=回到顶部><svg class="icon" viewBox="0 0 448 512"><path d="M34.9 289.5l-22.2-22.2c-9.4-9.4-9.4-24.6.0-33.9L207 39c9.4-9.4 24.6-9.4 33.9.0l194.3 194.3c9.4 9.4 9.4 24.6.0 33.9L413 289.4c-9.5 9.5-25 9.3-34.3-.4L264 168.6V456c0 13.3-10.7 24-24 24h-32c-13.3.0-24-10.7-24-24V168.6L69.2 289.1c-9.3 9.8-24.8 10-34.3.4z"/></svg></a></div><div id=cookieconsent-container></div><link rel=stylesheet href=/lib/katex/katex.min.0c8126645bb983a788b167b1b97abe2505a962ad45e049001463c46012012a9b.css integrity="sha256-DIEmZFu5g6eIsWexuXq+JQWpYq1F4EkAFGPEYBIBKps="><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/katex/copy-tex.min.cf8e3e934d92a839d209ffdac331fa693da5958a6dff2c8788a4713cc1f50a47.css integrity="sha256-z44+k02SqDnSCf/awzH6aT2llYpt/yyHiKRxPMH1Ckc="><noscript><link rel=stylesheet href=/lib/katex/copy-tex.min.cf8e3e934d92a839d209ffdac331fa693da5958a6dff2c8788a4713cc1f50a47.css integrity="sha256-z44+k02SqDnSCf/awzH6aT2llYpt/yyHiKRxPMH1Ckc="></noscript><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.cd0d0b6e50ff01ff2f3a9a70d7cfb66a7c6cb9acf7a566325568be6d3bd31fc4.css integrity="sha256-zQ0LblD/Af8vOppw18+2anxsuaz3pWYyVWi+bTvTH8Q="><link rel=stylesheet href=/css/profile.003e858ff1233d9b981c64a8f06ff6ef394ebd60f81a472e374eb195bb673c7d.css integrity="sha256-AD6Fj/EjPZuYHGSo8G/27zlOvWD4GkcuN06xlbtnPH0="><script>window.config={"autocomplete.min.js":"/lib/autocomplete/autocomplete.min.js",cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验."},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},data:{"desktop-header-typeit":"Daily Deep Think","id-3":"深度思考，持续学习","mobile-header-typeit":"Daily Deep Think"},"fuse.min.js":"/lib/fuse/fuse.min.js",math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{distance:100,findAllMatches:!1,fuseIndexURL:"/index.json",highlightTag:"em",ignoreFieldNorm:!1,ignoreLocation:!1,isCaseSensitive:!1,location:0,maxResultLength:10,minMatchCharLength:2,noResultsFound:"没有找到结果",snippetLength:50,threshold:.3,type:"fuse",useExtendedSearch:!1},twemoji:!0,typeit:{cursorChar:"|",cursorSpeed:1e3,data:{"desktop-header-typeit":["desktop-header-typeit"],"id-3":["id-3"],"mobile-header-typeit":["mobile-header-typeit"]},duration:-1,speed:100}}</script><script src=/lib/twemoji/twemoji.min.0e0e5259e3ff8ea805e0c5660c6336f7f46b14332e3cafb82939e1db3da8b6f8.js integrity="sha256-Dg5SWeP/jqgF4MVmDGM29/RrFDMuPK+4KTnh2z2otvg=" defer></script><script src=/js/twemoji.min.js defer></script><script src=/lib/typeit/typeit.min.06e0b9ba7bb3c9368aa26979037019306fef8e43dd2b9276854d227381445d0f.js integrity="sha256-BuC5unuzyTaKoml5A3AZMG/vjkPdK5J2hU0ic4FEXQ8="></script><script src=/lib/katex/katex.min.76d534cf1167067008fca12c4e903fc44cf8cfda8c5279c318d1f78cd90b086e.js integrity="sha256-dtU0zxFnBnAI/KEsTpA/xEz4z9qMUnnDGNH3jNkLCG4=" defer></script><script src=/lib/katex/auto-render.min.bb53eb953394531aae36fdd537065c4244eb8542901a3ce914601d932675b8ac.js integrity="sha256-u1PrlTOUUxquNv3VNwZcQkTrhUKQGjzpFGAdkyZ1uKw=" defer></script><script src=/lib/katex/copy-tex.min.07770af90943a1de1a1010794bc78c6a7346d46d48fb63e35cc76ba76b827604.js integrity="sha256-B3cK+QlDod4aEBB5S8eManNG1G1I+2PjXMdrp2uCdgQ=" defer></script><script src=/lib/katex/mhchem.min.9f87e5e9c384a160472d0045035a8641f6013358eddb3ece708634a50f946a40.js integrity="sha256-n4fl6cOEoWBHLQBFA1qGQfYBM1jt2z7OcIY0pQ+UakA=" defer></script><script src=/js/katex.min.js defer></script><script src=/lib/cookieconsent/cookieconsent.min.e55842a856a6d829feca3c3ad736c136b6c7549e9247274f78aa296259e06e24.js integrity="sha256-5VhCqFam2Cn+yjw61zbBNrbHVJ6SRydPeKopYlngbiQ=" defer></script><script src=/js/cookieconsent.min.js defer></script><script src=/js/theme.min.js defer></script><script type=speculationrules>
  {
    "prerender": [
      {
        "where": { "href_matches": "/*" },
        "eagerness": "moderate"
      }
    ]
  }
</script></body></html>