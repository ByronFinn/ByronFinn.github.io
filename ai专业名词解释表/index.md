# AI专业名词解释表


# AI专业名词解释表

本文档整理了AI大模型领域的核心专业术语，从基础概念到高级技术架构，帮助您系统性地理解人工智能技术体系。

---

## 📚 基础概念篇

| 名词 | 专业解释 | 通俗解释 | 举例说明 |
|------|----------|----------|----------|
| **AGI（通用人工智能）** | Artificial General Intelligence，具备人类水平智能的AI系统 | 能像人一样思考、学习、创造的全能AI | 一个能同时写诗、编程、做饭、聊天的机器人 |
| **LLM（大语言模型）** | Large Language Model，基于海量数据训练的大型神经网络模型 | 能理解和生成人类语言的"超级大脑" | GPT-4、Claude、文心一言等都是LLM |
| **训练** | 通过大量数据训练神经网络参数的过程 | AI的"学习阶段"，像人读书积累知识 | 用互联网所有文本训练一个模型学会语言 |
| **推理** | 训练完成的模型根据输入生成输出的过程 | AI的"应用阶段"，像人运用所学知识回答问题 | 输入问题后模型生成回答的过程 |
| **Token（词元）** | 模型处理文本的最小单元，通过分词算法切分的文本片段 | AI语言的"字粒子"，模型一个一个处理 | "我喜欢苹果" → ["我", "喜欢", "苹果"] |

---

## 🏗️ 架构技术篇

| 名词 | 专业解释 | 通俗解释 | 举例说明 |
|------|----------|----------|----------|
| **Transformer** | 基于自注意力机制的深度学习架构，2017年Google提出 | 现代AI的"神经骨架"，让模型高效理解语言 | GPT、BERT等所有大模型都基于Transformer |
| **Encoder（编码器）** | 将输入序列编码为语义表示的神经网络组件 | AI的"理解器"，把文字变成机器懂的向量 | BERT使用Encoder做文本理解任务 |
| **Decoder（解码器）** | 根据上下文逐token生成输出的神经网络组件 | AI的"写作器"，根据理解生成回答 | GPT系列都是Decoder-only模型 |
| **Self-Attention（自注意力）** | 计算序列中每个元素与其他元素相关性的机制 | AI自动"关注重点"，像人阅读时抓重点 | "银行"在"存钱"中关注"钱"，在"钓鱼"中关注"河" |
| **Multi-Head Attention（多头注意力）** | 并行多个自注意力机制，捕获不同类型的依赖关系 | AI从多个角度同时理解文本 | 一个头关注语法，另一个头关注语义 |
| **Positional Encoding（位置编码）** | 为每个token添加位置信息的向量表示 | 让模型知道"谁在前谁在后" | "我爱你"与"你爱我"意义不同 |
| **Query（查询向量）** | 主动查询相关信息的向量，表示当前词需要什么信息 | "我要找什么"的数字表达 | "苹果"查询相关的味道、颜色等属性 |
| **Key（键向量）** | 被查询信息的标识向量，表示每个词能提供什么信息 | "我能提供什么"的标签 | "甜"作为味道特征的Key，等待被查询 |
| **Value（值向量）** | 实际内容的表示向量，包含词的真实语义信息 | "我的具体内容"的数值化 | "甜"的实际语义表示[0.8, 0.2, -0.1] |
| **Attention Weight（注意力权重）** | 表示关注程度的重要性分数，通常通过softmax归一化 | "关注程度"的数值化 | 0.8表示强烈关注，0.1表示弱关注，所有权重和为1 |
| **Cross-Attention（交叉注意力）** | 不同序列间的注意力机制，Query来自一个序列，Key/Value来自另一个序列 | 跨模态信息交互 | 图文匹配中文字Query关注图像Key/Value |
| **Causal Attention（因果注意力）** | 只能关注当前位置及之前内容的注意力机制，防止未来信息泄露 | "只能向前看"的注意力 | GPT生成时第5个词只能看前4个词 |
| **Softmax Function** | 将任意实数向量转换为概率分布的激活函数 | 转换为"重要性百分比" | `[2,1,0] → [0.67,0.24,0.09]`，保持相对大小关系 |

---

## 🔢 数学表示篇

| 名词 | 专业解释 | 通俗解释 | 举例说明 |
|------|----------|----------|----------|
| **Vector（向量）** | 具有大小和方向的数学对象，一组有序数字 | 事物的"数字身份证"，用数字描述特征 | `[25, 180, 70]`可表示一个人的年龄、身高、体重 |
| **Embedding（嵌入）** | 将离散符号映射到连续向量空间的技术 | 把文字变成"数字坐标" | `"国王"→[0.25, -0.12, 0.78, ...]` |
| **Query / Key / Value** | 自注意力机制中的三个核心向量矩阵，分别代表查询需求、标识信息、实际内容 | Query=我要什么，Key=我能提供什么，Value=我的具体内容 | `Query=[0.1,0.2]`查询味道，`Key=[0.8,0.1]`标识甜味，`Value=[0.9,0.05]`甜味的实际表示 |
| **Feed-Forward Network（前馈网络）** | 对每个位置独立进行非线性变换 | 深化每个词的理解 | "春天"进一步联想到"温暖、生长" |
| **Layer Normalization（层归一化）** | 标准化层输入 | 训练"稳定器" | 防止梯度爆炸或发散 |
| **Residual Connection（残差连接）** | 跨层连接，保留原始信息 | 信息"直通车"，防止丢失 | 类似捷径路径避免深层网络退化 |

---

## 🔄 处理流程篇

| 名词 | 专业解释 | 通俗解释 | 举例说明 |
|------|----------|----------|----------|
| **Tokenizer（分词器）** | 将文本转换为token序列 | "文字切菜刀" | `"Hello world" → ["Hello", " world"]` |
| **Context Window（上下文窗口）** | 模型能处理的最大token数量限制 | AI的"记忆力上限" | GPT-4有128K上下文 |
| **Decoding（解码）** | 根据概率分布逐token生成文本 | AI"写字过程" | 从最可能的词开始生成 |
| **Temperature（温度参数）** | 控制生成随机性的参数 | "创意调节器" | 高温更有创意，低温更稳健 |
| **Top-p采样** | 基于累积概率的采样策略 | "精华筛选器" | 只考虑累计概率达到90%的候选词 |
| **Max Tokens（最大令牌数）** | 限制生成输出长度 | "字数限制器" | 防止AI回答过长 |

---

## 🛠️ 工程实践篇

| 名词 | 专业解释 | 通俗解释 | 举例说明 |
|------|----------|----------|----------|
| **RAG（检索增强生成）** | 结合检索和生成的AI方法 | "开卷考试"式AI | 先查资料再回答问题 |
| **Prompt Engineering（提示工程）** | 设计优化提示词的技术 | "说话艺术" | 让AI更好理解需求 |
| **Fine-tuning（微调）** | 在预训练模型上进行特定任务训练 | "定向培训" | 让通用模型变成医疗助手 |
| **BPE（字节对编码）** | 一种常见分词算法 | "文字压缩术" | `"unhappiness" → ["un","happi","ness"]` |
| **Detokenization（反分词）** | 将token序列还原为可读文本 | "拼字还原" | `["我","喜欢","苹果"]→"我喜欢苹果"` |
| **Streaming（流式输出）** | 逐token实时生成输出 | "打字机效果" | 聊天机器人边输出边思考 |

---

## 🧠 传统模型对比篇

| 名词 | 专业解释 | 通俗解释 | 举例说明 |
|------|----------|----------|----------|
| **RNN（循环神经网络）** | 逐步处理序列数据的神经网络 | "逐字阅读AI" | 翻译`"我爱你"`逐词处理 |
| **LSTM（长短期记忆网络）** | 改进型RNN，解决长期依赖问题 | "记忆力更强" | 能记住开头内容 |
| **CNN（卷积神经网络）** | 擅长处理图像模式的神经网络 | "图像专家" | 识别猫狗人脸 |
| **Encoder-Decoder架构** | 同时包含理解与生成模块的模型 | "全能型AI" | 机器翻译模型 |

---

## 📊 应用场景篇

| 名词 | 专业解释 | 通俗解释 | 举例说明 |
|------|----------|----------|----------|
| **对话产品** | 面向用户的AI应用接口 | "AI聊天壳" | ChatGPT、Claude |
| **API调用** | 程序间通信接口 | "AI电话线" | 程序调用OpenAI API |
| **上下文管理** | 维护对话历史的技术 | "AI记忆力" | 聊天机器人记住你说过的话 |
| **多轮对话** | 连续人机交互模式 | "连续聊天" | 先问天气，再问穿衣 |
| **工具调用（Function Calling）** | 模型可调用外部API执行任务 | "AI动手能力" | AI自动查天气或搜索资料 |

---

## 🧩 模型优化与训练技巧篇

| 名词 | 专业解释 | 通俗解释 | 举例说明 |
|------|----------|----------|----------|
| **LoRA（低秩适配）** | 通过低秩矩阵微调模型参数 | "轻量级微调" | 让LLM快速适应新领域 |
| **Quantization（量化）** | 用低精度表示模型参数 | "模型瘦身" | FP32→INT8加速推理 |
| **Pruning（剪枝）** | 删除冗余神经元或连接 | "修枝整形" | 去除无效参数 |
| **Distillation（知识蒸馏）** | 用大模型指导小模型学习 | "老师带学生" | GPT-4教小模型 |
| **Checkpoint（检查点）** | 模型训练中保存的中间状态 | "训练存档点" | 防止断电丢失进度 |

---

## 🔍 向量检索与知识集成篇

| 名词 | 专业解释 | 通俗解释 | 举例说明 |
|------|----------|----------|----------|
| **Embedding Model（向量模型）** | 将文本转为语义向量的模型 | "语义坐标机" | text-embedding-3-large |
| **Vector Database（向量数据库）** | 支持向量检索的数据库 | "语义仓库" | Milvus、Pinecone、FAISS |
| **Cosine Similarity（余弦相似度）** | 衡量两个向量方向相似度 | "语义相似度计" | `"猫在睡觉"≈"猫咪休息中"` |
| **Knowledge Graph（知识图谱）** | 用节点和关系存储知识结构 | "知识地图" | `"苹果→是→水果"` |
| **Hybrid Search（混合检索）** | 结合语义检索与关键词匹配 | "双保险搜索" | 同时检索`"猫"`和`"宠物动物"` |

---

## 🧩 多模态与智能体篇

| 名词 | 专业解释 | 通俗解释 | 举例说明 |
|------|----------|----------|----------|
| **Multimodal Model（多模态模型）** | 同时处理文本、图像、音频等模态 | "全感官AI" | GPT-4V、Gemini |
| **VLM（视觉语言模型）** | Vision-Language Model | "会看图的AI" | 看图问答AI |
| **Speech Recognition（语音识别）** | 将语音转文字 | "听写AI" | 语音输入法 |
| **TTS（文本转语音）** | 将文字转语音 | "AI播音员" | AI读出回答 |
| **AI Agent（智能体）** | 具备自主行动与决策能力的AI | "能动的AI助手" | Devin、AutoGPT |

---

## ⚙️ 模型评估与安全篇

| 名词 | 专业解释 | 通俗解释 | 举例说明 |
|------|----------|----------|----------|
| **Hallucination（幻觉）** | 模型生成虚假信息 | "一本正经胡说八道" | 编造论文或事实 |
| **Alignment（对齐）** | 模型与人类价值观对齐 | "价值观调教" | RLHF调教模型 |
| **RLHF（人类反馈强化学习）** | 用人类偏好优化模型 | "人教AI说话" | ChatGPT的训练方式 |
| **Red Teaming（红队测试）** | 对抗性测试模型安全 | "安全渗透测试" | 测试模型是否泄密 |
| **Bias（偏差）** | 模型输出的系统性偏见 | "AI偏心" | 对性别或语言偏好 |

---

## 🧰 新兴趋势与未来方向篇

| 名词 | 专业解释 | 通俗解释 | 举例说明 |
|------|----------|----------|----------|
| **Mixture of Experts（专家混合）** | 包含多个子模型动态激活结构 | "专家组AI" | `Gemini 1.5 Pro`架构 |
| **Context Compression（上下文压缩）** | 压缩历史对话节省token | "记忆压缩" | 长对话摘要 |
| **Memory-Augmented Model（记忆增强模型）** | 结合长期记忆机制的AI | "有记忆的AI" | `ChatGPT`长期记忆功能 |
| **Autonomous Agent（自主智能体）** | 能自我规划执行任务的AI | "自理AI" | `AutoGPT`、`Devin` |
| **Synthetic Data（合成数据）** | 由AI生成的虚拟训练数据 | "AI自制教材" | 用AI扩充训练集 |

---

## 💡 学习建议

### 🎯 核心概念掌握优先级

1. **入门级（必掌握）**：Token、Embedding、Transformer、LLM
2. **进阶级（重要）**：Self-Attention、RAG、Context Window
3. **高级（可选）**：LoRA、Mixture of Experts、Red Teaming

### 📖 学习路径建议

1. **理解基本原理**：Token是什么，为什么需要向量表示
2. **掌握核心架构**：Transformer的Encoder-Decoder结构
3. **实践应用技巧**：Prompt工程与RAG结合
4. **深入技术细节**：注意力机制与对齐训练

### 🔗 概念关联图

```
基础概念 → 数学表示 → 架构技术 → 处理流程 → 工程实践 → 优化 → 检索 → 智能体
↓         ↓         ↓         ↓         ↓         ↓       ↓         ↓
Token   → Vector   → Transformer → Decoding → RAG → LoRA → Embedding → Agent
LLM     → Q/K/V    → Attention  → Context → Prompt → Quant → Knowledge → Memory
```

---

> 🚀 **提示**：AI技术体系庞大但高度关联。建议从"理解→实现→优化→安全"四个维度系统学习。

---

## 📘 附录：AI专业术语中英对照速查表（A–Z Glossary）

| 英文缩写 / 术语 | 中文名称 | 简要说明 |
|------------------|-----------|-----------|
| **AGI (Artificial General Intelligence)** | 通用人工智能 | 具备人类水平通用智能的AI |
| **Alignment** | 对齐 | 让AI行为符合人类价值观的过程 |
| **API (Application Programming Interface)** | 应用程序接口 | 程序间通信调用的标准方式 |
| **AutoGPT / Autonomous Agent** | 自主智能体 | 能自主规划和执行任务的AI系统 |
| **BERT (Bidirectional Encoder Representations from Transformers)** | 双向Transformer编码模型 | 代表性的NLP预训练模型 |
| **Bias** | 偏差 | 模型输出中的系统性不公平 |
| **BPE (Byte Pair Encoding)** | 字节对编码 | 常用的文本分词算法 |
| **Checkpoint** | 检查点 | 模型训练过程中保存的中间状态 |
| **CNN (Convolutional Neural Network)** | 卷积神经网络 | 擅长图像识别的网络结构 |
| **Context Window** | 上下文窗口 | 模型可处理的最大token数量 |
| **Context Compression** | 上下文压缩 | 对历史内容进行摘要以节省上下文 |
| **Cosine Similarity** | 余弦相似度 | 衡量向量间语义相似度的指标 |
| **Decoder** | 解码器 | 将语义向量生成文本的网络模块 |
| **Decoding** | 解码过程 | 模型生成文本的过程 |
| **Detokenization** | 反分词 | 将token序列还原为文字 |
| **Distillation (Knowledge Distillation)** | 知识蒸馏 | 大模型指导小模型学习的技术 |
| **Embedding** | 嵌入 | 将离散词语映射到连续向量空间 |
| **Embedding Model** | 向量模型 | 生成文本语义向量的模型 |
| **Encoder** | 编码器 | 将文本转换为语义表示的网络组件 |
| **Encoder–Decoder** | 编码–解码结构 | 同时具备理解与生成能力的模型架构 |
| **Feed Forward Network (FFN)** | 前馈网络 | Transformer层内的非线性变换模块 |
| **Fine-tuning** | 微调 | 基于预训练模型进行特定任务再训练 |
| **Function Calling** | 工具调用 | 模型调用外部API执行操作的能力 |
| **Hallucination** | 幻觉 | 模型生成虚假或编造信息的现象 |
| **Hybrid Search** | 混合检索 | 结合语义检索与关键词搜索的技术 |
| **Knowledge Graph** | 知识图谱 | 用节点和关系结构化存储知识的网络 |
| **Layer Normalization** | 层归一化 | 网络层输入的标准化过程 |
| **Latency** | 延迟 | 模型从输入到输出的响应时间 |
| **LLM (Large Language Model)** | 大语言模型 | 基于大规模语料训练的语言模型 |
| **LoRA (Low-Rank Adaptation)** | 低秩适配 | 轻量级模型微调方法 |
| **LSTM (Long Short-Term Memory)** | 长短期记忆网络 | 能捕获长距离依赖的RNN变体 |
| **Memory-Augmented Model** | 记忆增强模型 | 具备长期记忆能力的AI |
| **Mixture of Experts (MoE)** | 专家混合模型 | 动态选择多个子模型协作的架构 |
| **Multi-Head Attention** | 多头注意力 | 并行计算多种注意力的机制 |
| **Positional Encoding** | 位置编码 | 为token添加位置信息的方式 |
| **Pruning** | 剪枝 | 删除冗余参数减小模型规模 |
| **Prompt Engineering** | 提示工程 | 优化提示词以提升模型输出质量 |
| **Quantization** | 量化 | 用低精度表示模型参数以提升性能 |
| **Query / Key / Value (QKV)** | 查询 / 键 / 值 | 自注意力机制的三要素 |
| **RAG (Retrieval-Augmented Generation)** | 检索增强生成 | 将外部知识检索与生成结合的技术 |
| **Red Teaming** | 红队测试 | 通过对抗输入评估模型安全性 |
| **Residual Connection** | 残差连接 | 跨层信息直通结构，防止梯度退化 |
| **RLHF (Reinforcement Learning from Human Feedback)** | 人类反馈强化学习 | 通过人类偏好优化模型输出 |
| **RNN (Recurrent Neural Network)** | 循环神经网络 | 逐步处理序列数据的网络结构 |
| **Self-Attention** | 自注意力 | 计算序列中元素相关性的机制 |
| **Streaming** | 流式输出 | 模型边生成边输出的方式 |
| **Synthetic Data** | 合成数据 | AI生成的虚拟训练数据 |
| **Temperature** | 温度参数 | 控制生成随机性的参数 |
| **Throughput** | 吞吐量 | 每秒处理的请求数量 |
| **Token** | 词元 | 模型处理文本的最小单位 |
| **Tokenizer** | 分词器 | 将文本拆分为token的工具 |
| **Top-p Sampling** | 累积概率采样 | 过滤低概率词汇的生成策略 |
| **Transformer** | Transformer架构 | 基于注意力机制的核心神经网络 |
| **TTS (Text-to-Speech)** | 文本转语音 | 将文字转为自然语音 |
| **Vector** | 向量 | 数字化表示实体特征的数学结构 |
| **Vector Database** | 向量数据库 | 存储并按语义检索向量数据的系统 |
| **VLM (Vision-Language Model)** | 视觉语言模型 | 同时理解图像与语言的模型 |
| **Weight** | 权重参数 | 模型中可学习的核心数值参数 |

---
